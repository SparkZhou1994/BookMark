# 存储引擎
## 简述概述
### 数据库架构
1. 传输模块
	集群通信，客户端通信
2. 查询处理器
	查询解析器，查询优化器
3. 执行引擎
	远程执行，本地执行
4. 存储引擎
	事务管理器，锁管理器
	访问方法
	缓冲区管理器，恢复管理器
### 数据布局
行 列 宽列式存储
### 缓冲 不可变性 有序性

## B树基础知识
机械硬盘以扇区为单位，固态硬盘以块为单位，操作系统抽象出块的概念。
固态硬盘通过 闪存转换层（Flash Translation Layer）负责映射、跟踪、清理块。
硬盘结构中创建长依赖链会增加代码复杂性，最好将指针数量和跨度保证最小。
B树高扇出和减少平衡操作

## 文件格式
原地更新还是仅追加都是以页为单位，一旦内存中的页写满了，就将其刷写到硬盘中。
### 页的结构
一连串的三元组,pkv,p表示指向子页的指针，k表示键，v表示关联的值
缺点：
1. 在中间插入键，需要移动已有元素
2. 只能用于定长数据
### 分槽页
我们将页组织成一个槽，将指针和单元格分别存放在页两侧的单独空间。修改数据时，只需改动指针；如果删除数据，可以删除指针或将指针置空。
### 单元格布局
单个页中的所有单元格都是统一的，全是键单元格或者全是键值单元格
键单元格： key_size, page_id, key
键值单元格: flags, key_size, value_size, key, data_record
### 单元格放入分槽页
单元格按插入顺序排列，偏移量按逻辑顺序排列。
### 管理变长数据
删除记录时，将单元格标记删除，更新内存中可用列表。
如果找不到够长的连续字节来存放新的单元格，但有足够多的碎片字节可用，我们会读出所有存活的单元格再重新写入，如果依旧没有足够的空间，创建一个溢出页。（溢出页的管理？？？指针位数是否够？？？）
页标示符用于在树文件中定位子节点，而单元格偏移量用于在页内定位单元格。
### 校验和
读取数据需要比较确保数据没有被损坏。以页为单位，并写入页头部。

## B树的实现
### 溢出页
记录从主页指向溢出页，数据是连续的。如果单个溢出页不够，通过在前一个溢出页的头部保存下一个溢出页的ID，将多个溢出页链接在一起。
键通常具有很高的基数，大多数比较通过键来进行。如果所有数据记录都过大，值得考虑blob存储。
### 导航信息
如果节点分裂或合并，可以使用导航信息上拉到父节点的键查找插入点
### 再平衡（负载平衡）
为了提高空间利用率，不在溢出时拆分节点，而是转换到同级的另一个节点；在删除过程中，选择相邻节点移动元素，而不是合并同级节点。
### 仅在右侧追加
主键自增
### 批量加载
在叶子层上按页写入预排序数据，所有的分裂都发生在最右节点上。

## 事务处理与恢复
页缓存充当了持久性存储和存储引擎其余部分之间的中介。一切数据库状态的更改都首先应用在缓存的页上。
### 缓冲区管理
#### 缓存回收
为了确保所有更改都被持久化，检查点进程会协调刷写进程。检查点进程控制预写日志（WAL）和页缓存。只有当缓存页完成刷写之后，相关操作的日志记录才能从WAL中丢弃。
#### 恢复
大多数数据库实现了模糊检查点，日志的头部last_checkpoint记录了最后一次成功的检查点信息。以begin_checkpoint开始，以end_checkpoint结束。
使用影子页（copy on write），使用物理日志保存完整页状态和字节级的更改（完整记录），用逻辑日志保存状态上的执行（哪些操作）。
steal策略是在事务提交前允许刷写修改过的页。
force策略要求事务提交前将事务修改过的页刷写到磁盘上。
撤销操作会回滚那些已经提交事务强制刷盘的页上的更新；重做将已提交事务执行的更改应用到磁盘上。
如使用no-steal策略，那么只需要重做日志就可以实现恢复；如果使用no-force策略，通过推迟对页的更新来缓冲他们，但需要一个大的页缓存。
我们需要在日志中保留撤销信息，否则保留重做日志，当撤销或重做记录被写入日志文件后，事务才能提交。
#### 并发控制
乐观并发控制 OCC，提交前检查历史操作是否冲突，如果有则中止其中某个冲突的事务
多版本并发控制 MVCC，一条记录同时存在多个时间戳版本，使用验证技术，允许多个更新或事务提交中的某个获胜，也可以用无锁技术（时间戳排序），或锁技术（二阶段锁）、
悲观并发控制 PCC，基于锁和无锁，无锁根据未完成事务的调度，维护读取与写入的操作列表以限制事务的执行。
如果一个调度等效于同一组事务的某一完整串行调度，则该调度是可串行化的。
隔离级别 read uncommitted、read committed、reapeatable read、serializability
快照隔离，每个事务都有一个自己的副本，只有事务中修改的值没有被其他事务修改，才能提交，否则中止并会滚。但这样依旧会产生写倾斜异常。
乐观锁并发控制，分为三个阶段，读阶段、验证阶段（确实事务是否遵循ACID性质）、写阶段。验证阶段和写阶段都应该具有原子性，验证事务时，其他事务不允许提交。读些锁、锁升级。
多版本并发控制，事务管理器的目标是确保任一时刻最多只有一个未提交版本。加锁、调度、冲突解决技术、时间戳排序。
悲观并发控制，时间戳排序是最简单的实现方式。事务管理器存在max_read_timestamp和max_write_timestamp。时间戳小于read的写操作与最近的读操作冲突，但这是被允许的（托马斯写规则）。中止的事务会以最新的时间戳重新开始。
锁的并发控制，二阶段锁，1. 增长阶段，获取所有锁，而不释放； 2. 收缩阶段，释放增长阶段的所有锁。事务一旦释放锁，就不能再获取其他锁。二阶段锁和二阶段提交是两个概念。
死锁处理，最简单的方式增加计时器，或者保守2PL，事务执行前不能获取所有锁，如果不能则中止事务。为了避免死锁，事务管理器可以用时间戳来确定事务优先级。优先级低的事务不能获取优先级高的事务的锁。【等待-死亡】，反之【伤害-等待】。
负责逻辑和物理完整性的是通过lock和latch实现的，lock用于隔离和调度重叠的事务，在键上获取。lock比latch更重量级，在事务执行期间一直存在。latch在页上获取，无锁并发也必须使用latch。
latch耦合，读取时获取子节点的latch就释放父节点的latch；插入时，如果子节点没满就释放父节点latch；删除时，子节点还有很多元素，就释放父节点latch。
latch升级，如果叶节点需要分裂或合并，沿树向上，将父节点的共享锁变为排他锁。未抢到锁的线程需要等待或者重试。
blink树，即使子节点分裂，也无需保留父节点的锁，可以通过同级链接指针使新节点可见，惰性更新父节点指针。好处是读操作可以和树结构变化并发进行，防止死锁，并发的修改在向父节点传播时，可能引起死锁。

## B树的变体
### 写时复制，读取者不需要同步，不会阻塞写入者，任何操作都不会得到一个处于不完整状态的页。LMDB是写时复制的一个实现。
### 惰性B树， WiredTiger通过缓冲，来记录写。单个节点；惰性自适应树，更新缓冲区将跟踪对子树顶部节点及其后代节点所执行的所有操作，缓冲区具有层次依赖关系，级联的。
### FD树 写入不需要定位目标，所有更新都只是追加操作。
#### 多段级联 我们可以创建一个从高层数组的每个元素到下一层最近的元素的映射，为了简化搜索，我们可以通过在序号较大的数组中每隔一个元素就将元素拉到序号较小的数组来弥补元素间的间隙。可以在上层使用二分查找，减少搜索成本。
#### 对数级的有序段，其中来自较低层页的头元素被传播为指向较高层的指针，降低搜索成本。FD树不在原地更新页，可能在几个层上存在相同键的数据记录，所以FD树通过插入墓碑来进行删除。该墓碑指示与相应键关联的数据记录已经删除，必须丢弃较低层该键的所有数据记录，当到最底层时表示没有需要他们屏蔽的数据项了。
### Bw树 Bw树不是在写入时获取独占所有权，而是对映射表中的物理偏移量使用CAS操作。如果有两个线程试图将一个增量节点放置在同一个逻辑节点上，只有一个会成功。Bw树逻辑结构类似B树，依旧需要SMO，在SMO时放置一个附加的中止增量节点，防止分裂和合并，在完成SMO时，即可删除。OpenBw树是其实现。写放大或缩小并不频繁，所以不加latch来保护。
### 缓存无关B树，van Emde Boas树，在逻辑上的布局表示（节点形成树），而在底部，可以看到如何在内存和磁盘的布局。为了使数据结构动态化，使用打包数组，使用连续内存存储元素，但在之间保留间隙。

## 日志结构存储
从高层级看来，存储结构的内部与外部，对待数据的方式有严格的区别。在内部，不可变文件可以保存多个副本，更新的副本将覆盖旧的副本，而可变文件通常只保存最新的值。当被访问时，不可变文件被处理，冗余副本需要进行协调，最新的副本会返回客户端。
我们使用B树作为可变数据结构的典型示例，而使用日志结构合并树（LSM）作为不可变结构的示例
### LSM树
在不同的存储结构中有两种应用缓冲的方式：推迟对磁盘驻留页的写入传播，以及使用操作顺序化。
LSM由于它的不可变性，需要使用类似于归并排序的方法合并树的内容。这一过程会发生在回收冗余副本所占空间的维护期也会发生在向用户返回内容之前的读取期。
LSM树在写远大于读的应用程序中特别有用。读写在设计上并不交叉，因此可以在没有段级锁的情况下读写磁盘上的数据，这大大简化了并发访问。
B树与LSM树都需要内务处理（housekeeping）来优化性能，LSM树必须合并和重写文件，因为请求的数据记录可能分布在多个文件中，而B树可能必须被部分或全部重写，以减少碎片并回收被更新或删除的记录所占的空间。
#### LSM树的结构
LSM树由较小的内存驻留组件和较大的磁盘驻留组件组成。内存驻留组件是可变的。其大小达到一个可配的阈值时，将被持久化到磁盘中。它的更新不需要磁盘访问。需要一个单独的预写日志文件保证数据记录的持久性。在用户确认操作之前，数据记录会被追加到日志汇总并提交到内存。所有读写都应用于一个内存驻留表，该表维护一个允许并发访问的有序数据结构（内存排序树）。磁盘驻留件通过内存中缓冲的内容刷写到磁盘来构建的，仅用于读取。
双组件LSM树，只有一个磁盘组件，由不可变段组成，这里的磁盘组件被组织成B树。内存驻留树的内容被分块刷写到磁盘中，将内存驻留段和磁盘驻留子树的合并内容写入磁盘的新阶段。刷写成子树后，原来的内存驻留子树和磁盘驻留子树都被丢弃，合并通过同时步进两个迭代器来实现。由于写放大特性，合并相对频繁，所以没有被很好的实现。
多组件LSM树
不止一个磁盘驻留表，因此不总能确切知道哪些表持有我们所需的数据记录，所以我们不得不搜索多个文件夹来定位数据。我们通过压实周期性合并来将表的数量维持在最少。
当前memtable(读写)->正在刷写的memtable(读)->刷写目标(不可访问)->已刷写的表(读)->正在被压实的表(读)->已压实的表(读)
当memtable内容被完全刷写到磁盘上时，日志可以被修剪，并且保存对被刷写过的memtable进行操作的日志段可以被丢弃。
#### 更新与删除
需要墓碑来标记，否则数据可能复活。有时，删除连续范围而不是单一的键可能很有用，这可以使用谓语删除来完成，称为墓碑范围。
#### 合并迭代
磁盘驻留表的内容都是有序的，使用多路归并排序。通常，存储引擎提供游标和迭代器来实现遍历。游标保存上次消耗的数据记录的偏移量。多路归并排序采用一个优先级队列，如最小堆。每当其中一个迭代器耗尽时，就不再插入这个迭代器的值，但算法继续，直到所有迭代器都耗尽。
#### 协调
不同的表可能持有相同键的数据记录，为了协调数据记录，我们需要了解谁是优先的，可以通过时间戳。被更高时间戳屏蔽的记录不会返回给客户端，压实期间也不会被写入。
#### LSM的维护
墓碑表示了正确协调所需的重要数据，因为某些其他表还保存着墓碑所屏蔽的过期数据记录，在压实过程中，墓碑不会被立即丢弃，直到存储引擎可以确定在任何其他表中都不存在时间戳更小的相同键的数据记录。
分层压实将磁盘驻留表分为多个层级。0层表示通过memtable的内容而创建的，可能包含重叠的键范围。1层及其以上的层不包含重叠键范围，因此，将0层表分区，分裂成多个范围，将相同键进行合并。压实可以读取所有0层表和1层表，并输出分区过的1层表。每一层都有表大小和个数限制。一旦表的数量达到一个阈值，来自当前层的表与键范围重叠的下一层的表进行合并。
按大小分层压实，按照大小进行分组，小的和小的进行压实。0层要么是从memtalbe刷写的，要么是通过压实过程来创建的。会产生表饥渴，小的表一直在压实，大的表一直不被回收，增加读取的开销。这情况对于下一层将不得不进行强制压实。
其他压实策略，如针对不同的工作负载进行优化————时间窗口压实策略，允许整个丢弃那些已过期时间范围的数据文件，而不是对他们的内容进行压实和重写。
## 读写放大和空间放大
RUM猜想 Read Update Memory 如果减少两项开销必将增大另外一个开销。由于该模型没有考虑其他的重要指标，只能初步评估或者当作一个经验法则。
### 实现细节
#### 有序字符串表 SSTable
磁盘驻留表通常使用有序字符串表来实现。SSTable通常使用索引文件和数据文件，索引文件通常采用对数时间复杂度（B树）和常数时间复杂度（哈希表）
当memtable被刷写时，它的内容被写入磁盘，此时二级索引文件与SSTable主键索引也一起被创建出来。由于LSM树在内存中缓冲数据，索引必须也在驻留内存和驻留磁盘上有效，所以SASI维护了一个单独的内存结构，为memtable建立索引。
#### 布隆过滤器
布隆过滤器（用于集合成员判断）、HyperLogLog（用于估计基数【找出集合中不同元素的数量】）、CountMin Sketch（用于估计频率）
#### 跳表
跳表的概率复杂度保证与搜索树接近，不需要插入或者更新而旋转或者移动元素，而是使用概率性平衡。跳表节点很小，在内存中时随机分配的。有些实现通过使用松散链表来对这种情况进行改进。
我们可以通过实现一个线性化方案来创建一个跳表的并发版本，该方案使用一个额外的fully_linked标志来确定节点指针是否被完全更新。在具有非托管内存模型的语言中，可以使用引用计数或危险指针来确保并发。这种算法是无死锁的，因为节点总是从更高层开始访问。
#### 磁盘访问
LSM树的实现依赖于页缓存，用于磁盘访问和中间缓存。LSM树中的数据记录不一定是页对齐的，有些记录跨越页边界，指针可以使用绝对偏移量而不是页ID来实现寻址。
#### 压缩
在压缩和刷盘期间，压缩的页被顺序地追加，并且压缩信息存储在单独的文件段中。在读取过程中，将压缩后的页偏移量及其大小查询出来，之后在内存中解压压缩并物化。
### 无序LSM存储
无序存储一般不需要单独的日志，并且允许我们按插入顺序存储数据记录以减少写入的开销。
#### Bitcask
不使用memtable进行缓冲，而是将数据记录直接存储在日志文件中。为了使值可被搜索，使用一种名为keydir的数据结构，它保存与键相应的最新数据记录的引用。由于在任何给定时刻都只能有一个值与keydir中的键相关联，所以点查询不必合并来自多个数据源的数据。数据记录直接存储在日志文件中，因此不必维护单独的预写日志，这减少了空间开销和写放大。这种方法的缺点是它只支持单点查询，不允许范围扫描，因为数据项在keydir和数据文件中都是无序的。
#### WiscKey
vlog文件保存无序数据记录。键存储在排序的LSM树中，指向日志文件中最新的数据记录。由于键通常比与其相关的数据记录小得多压实效率也高很多。这种方法对于更新和删除较少的场景特别有用，在这种情况下，垃圾收集不会释放太多的磁盘空间。主要挑战是，由于vlog是未排序的，范围扫描需要随机IO。vlog是无序的，且不包含存活信息，必须扫描键树一查找哪些值仍然是存活的。
### LSM树中的并发
写入操作在memtable中被缓冲，并且它们的内容在完全刷盘之前是不具备持久性的，因此日志截断必须与memtable刷盘相协调。刷盘一旦完成，日志管理器就会得到最新的已刷写日志段的信息，可以安全地丢弃其中的内容。
### 日志堆叠
LSM树和固态硬盘是一个很好的搭配。
#### 闪存转换层
单个页不能被擦除，只有快中的页组才能被一起擦除。总结起来，固态硬盘使用日志结构存储的动机是将小的随机写入缓冲在一起进行批处理以均摊IO成本。
#### 文件系统日志记录
日志堆叠以几种不同的方式体现。首先，每一层都必须进行该层自己的信息记录，底层日志不会暴露必需的信息给上层，从而避免重复工作。由于各层之间不会沟通与LSS相关的调度（丢弃或重新定位段），所以低层子系统可能对丢弃的数据或即将丢弃的数据执行冗余操作。
一些数据库厂商建议将日志保存在单独的设备上，以隔离工作负载，并能够独立地考虑它们的性能和访问模式。更重要的是保持分区与底层硬件的对齐，并保持写入与页大小的对齐。
### LLAMA与精心堆叠
LLAMA中的Bw树感知可以将几个增量节点合并到单一的连续物理位置。并进行逻辑合并。
垃圾收集不仅可以回收可用空间，而且可以显著减少物理节点的碎片。如果垃圾收集只是连续重写了几个增量节点，它们仍将占用相同数量的空间，并且读取者还需要执行将增量更新应用到基节点。如果较高层的系统合并了节点并将它们连续地写入新位置，则LSS仍然不得不对旧版本进行垃圾收集。
### 开放通道固态硬盘
堆叠软件层的另一种选择是跳过所有间接层而直接使用硬件。实现之一是基于LSM树的KV存储，其二是LightNVM，它是在Linux内核中实现的。
内存转换层（FTL）通常处理数据分布、垃圾收集和页移动。开放通道固态硬盘越过FTL直接暴露器内部信息、驱动器管理和I/O调度。
软件定义闪存（SDF）是一种软硬件协同设计的开放通道固态硬盘系统，读写单元的大小不同，并且写单元大小对应于擦除单元大小，降低了写放大，对于日志结构存储非常理想。

# 第一部分总结
				缓冲  可变性 	有序性
B+树 			否 	 是 		是
WiredTiger 		是 	 是 		是
LA树 			是 	 是 		是
COW B树 			否 	 否 		是
2C LSM树 		是 	 否 		是
MC LSM树 		是  	 否 		是
FD 树 			是 	 否 		是
Bitcask 		否 	 否 		否
WiscKey 		是 	 否 		是
Bw 树 			否  	 否 		否

增加内存缓冲总是对写放大有积极的影响，原地更新的数据结构中，内存缓冲通过合并多个相同页的写入来均摊开销，有助于减少写放大。使用不可变性可能会将写放大延迟。不可变性对并发性和空间放大有积极影响。

# 分布式系统
进程可以用时钟来获取时间，它们可以是逻辑的，也可以是物理的。逻辑时钟是单调递增的计数器实现的。物理时钟也被称为挂钟通过操作系统获取。
## 简述与概述
### 分布式计算的误区
#### 时钟和时间
从时间感知不同的参与者收集和聚合数据时，你必须了解它们之间的时间漂移并相应地对时间进行归一化，而不是依赖源时间戳。
Spanner使用特殊的时间API，该API返回时间戳和不确定性界限以施加严格的事务顺序。
### 双将军问题时对分布式系统一致性的著名描述之一。实验表明如果链路可能发生故障并且通信是异步的，则不可能再通信的双方之间达成共识。
### FLP不可能问题，表明不存在任何协议能保证在有限时间内达成共识。
### 故障类型
#### 崩溃故障
崩溃-停止。
崩溃-恢复，崩溃-恢复需要考虑所有可能的恢复状态。
#### 遗漏故障
可能由间歇性停顿，网络过载，队列满引起。
## 故障检测
故障可能发生在链路层或者进程层。将活的进程标记为死（假阳性），或者将无响应进程标记为死并期待它做出响应（假阴性）。
活动性和安全性时描述算法解决特定问题的能力及其输出正确性的属性。活动性时一种保证特定预期事件必须发生的属性。安全性保证意外事件不会发生。
效率和精准度都具备的故障检测器是不可能的存在的。
### 心跳和ping
#### 无超时的故障检测器
每个进程维护一个邻居列表和与其相关联的计数器。进程向邻居发送心跳信息，每个信息都包含到心跳到目前为止所经过的路径。一个发件人和唯一标识符用于表示信息防止同一个信息被广播多次。
该方法有个缺点：需要选择一个能够产生可靠结果的阈值，否则会错误地将活动进程标记为死亡。
#### 外包心跳
利用的是从其相邻进程的角度查看到的进程活动性信息。该方法不需要进程知道网络中所有其他进程，只需要知道其连接的对等进程的子集。
### phi增量故障检测器
用连续范围来捕获被监视进程崩溃的概率。工作方式是维护一个滑动窗口，从对等进程收集最近心跳的到达时间。该信息用于估算下一个心跳的到达时间，与实际到达时间进行比较得出可疑程度。如果该值达到阈值则标记为宕机。
### Gossip和故障检测
每个成员维护一个其他成员的列表：它们的心跳计数器和时间戳。将其列表分发给随机相邻节点，收到消息后，进行合并和更新。任何节点在足够长的时间里没有更新其计数器，就认为他发生了故障。
### 反向故障检测
将所有活动进行分组，每次检测到单个进程故障时，它被转换并传播为组故障。组中的进程定期向其他成员发送ping信息。

## 领导者选举
选举算法的活动性保证了大多数时间会有一个领导者，选举最终会完成（系统不应该无限期地处于选举状态）
理想情况下，我们也希望获得安全性，保证一次最多只能有一个领导者，并完全消除脑裂的可能性。但实践中，许多算法违反这一协定。
领导者进程实现广播中消息的全序。领导者收集并保持全局状态。
选举必须是确定性的：选举过程中必须只产生一个领导者。
如果分布式锁算法对某个进程或进程组有偏好，不被骗好的进程最终将产生对共享资源的饥饿，这与活动性矛盾。具有领导权的系统中，领导者可能成为瓶颈，为此许多系统将数据划分在不相交的独立副本集中。每个副本集都有自己的领导者，而不是在整个系统范围上使用一个领导者。Spanner就是其中一个实现。
使用临时领导者来减少参与者达成一致所需的信息数量。
### 霸道选举算法
使用进程排名来认定新的领导者
1. 3发现6崩溃，向高排名的进程发送选举消息
2. 4、5响应
3. 3让5当选
4. 5广播Elected（选举完成）消息
算法有个明显问题，在网络分区的情况下，违反了安全性保证（一次最多只能选举一个领导者）节点被分成两个或多个独立运行的子集，每个子集选出了这个子集的领导者。这个被称为脑裂。
对排名较高节点有强烈的偏向性，如果节点不稳定，可能导致算法一致处于重新选举状态。
### 依次故障转移
每个选举出的领导者都提供一个故障转移节点列表。当其中一个进程检测到领导者故障时，它通过向列表中排名最高的备选进程发送消息来发起选举。如果故障进程时排名最高进程，它可理解通知其他进程新领导者信息。
### 候选节点/普通节点优化
将节点分为两个子集，候选节点和普通节点。为了解决同时发生多个选举问题，使用一个特定于进程的延迟变量，使得其中一个节点可以在其他节点之前发起选举，高优先级的节点具有较低的延迟变量。
### 邀请算法
邀请其他进程进入它们的组，每个组都有自己的领导者，算法允许多个领导者存在。
1. 四个进程各有一名成员的组的领导者，1邀请2，3邀请4
2. 1作为第一组的领导者，3成为另一个组的领导者
3. 两者合并，1成为领导
为了将合并组所需的信息数量保持在最低限度，较大组的领导者可以成为新组的领导者。邀请算法允许创建进程组并合并它们，而不必从头开始触发新的选举，减少了完成选举所需的信息数量。
### 环算法
系统中所有的节点形成一个环，并且知道环拓扑。当进程检测到领导者故障时，它发起新的选举。选举信息沿着环向下转发，如果该节点不可用，则进程跳过不可达的节点，并尝试联系环资后的节点，直到最终有一个节点响应为止。
由于环可以有两个或更多的部分，因此会出现脑裂。
### 本章小结
领导者的身份可能在其他进程不知晓的情况下发生变化，本地进程对领导者是否有效的问题已经存在。为了解决这个问题，我们需要将领导选举和故障检测结合一起。如稳定领导者选举，该算法具有唯一稳定领导者的回合和基于超时的故障检测。拥有一个领导者是确保活动性的一种方式。安全性的缺失和允许多个领导者是一种性能优化，算法可以继续进行复制阶段，而安全性则通过检测和解决冲突来保证。

## 复制和一致性
在具有单一真相来源的系统中，将副本晋升为新的主数据库。有些系统则不需要显式的重新配置，通过在读写查询期间收集多个参与者的响应来确保一致性。
当数据记录被修改时，其副本必须被相应地更新。在谈论复制时，我们最关心这三件事件：写入、副本更新和读取。客户端必须能够以特定的顺序观察到发生过的操作。
### 臭名昭著的CAP理论
一致性和分区容忍系统（CP系统）CP系统更倾向拒绝请求，而不是提供可能不一致的数据。
可用性和分区容忍系统（AP系统）AP系统放松了一致性要求，允许在请求期间提供可能不一致的值。
#### 小心使用CAP
CAP讨论的是网络分区，而不是节点崩溃或任何其他类型的故障（如奔溃恢复）。与集群其他节点分隔开的节点可以做出不一致的响应，但崩溃的节点根本不会响应。
CAP意味着，即使所有节点都启动了，只要它们之间有连接性问题，我们仍可能面临一致性问题。
CAP中的一致性定义与ACID定义的一致性完全不同。ACID一致性描述了事务一致性：事务将数据库从一个有效状态带到另一个有效状态，保持所有数据库的约束（如唯一性约束和引用完整性）。而在CAP中，它意味着操作是原子的和一致的。
如果使用正确，声称具备可用性的数据库仍然能够提供来自副本的一致结果，前提是要存在足够多的活着的副本。当然，还有更复杂的故障场景，CAP猜想只是一条经验法则，并不一定能说明全部事实。
#### 瘦成与产量
收成定义查询的完成程度，如要返回100行，只返回99行，比不返回内容好。
产量指成功完成的请求数与尝试请求总数之比。如一个繁忙的节点没有宕机，但仍然可能无法响应某些请求。
这就把权衡的重点从绝对条件变成了相对条件。我们可以用收成换取产量，并允许某些请求返回不完整的数据。
### 顺序
每个进程按顺序执行自己的一组操作。顺序进程执行的组合形成了一个全局历史，这其中，操作可能并发执行。
为了推理操作顺序并明确描述可能的结果，必须定义一致性模型。我们用共享内存和并发系统来讨论分布式系统中的并发性，因为大多数一致性定义和规则仍然是适用的。尽管并发系统和分布式系统之间存在着许多重叠的术语，但由于通信方式、性能和可靠性等方面的差异，我们无法直接应用大多数并发算法。
### 一致性模型
#### 严格一致性
#### 可线性化
在分布式系统中，可线性化需要协调冲突和顺序。它可以使用共识算法来实现：客户端使用消息与复制存储进行交互，共识模块负责确保应用的操作在整个集群中是一致且相同的。每个写操作将在它的调用和完成事件之间的某个时间点瞬间出现且仅出现一次。
有趣的是，可线性化在其传统理解中被视为局部性质，并且意味着其是独立实现和验证的单元组合。合并可线性化的历史将产生一个同样可线性化的历史。换句话说，其中所有对象都是可线性化的系统，也是可线性化的。它的范围仅限于单个对象，而且即使对两个独立对象的操作是可线性化的，涉及两个对象的操作还必须依赖于额外的同步手段。
可重用可线性化基础设施是一种用于实现可线性化远程过程调用的机制。在RIFL中，消息用客户端ID和客户端本地单调递增的序列号唯一标识。为了分配客户端ID，RIFL使用由系统层的服务颁发的租约————规避重复序列号的唯一标识符。如果发生故障的客户端试图使用过期的租约执行操作，它的操作将不会被提交，客户端必须接收新的租约，然后重试。
系统需要防止重复执行重试操作。当客户端重试操作时，RIFL返回一个完成对象来指示与它相关联的操作已经被执行并返回结果，而不是再次应用操作。完成对象与实际数据记录一起存储在持久存储中。
#### 顺序一致性
顺序一致性常与可线性化混淆。两者要求操作的全局有序性，而可线性化要求每个过程的局部有序性与全局有序性一致。可线性化遵循操作的真实顺序，而在顺序一致性条件下，顺序仅适用于来源相同的写入。我们可以组合可线性化的历史，并且仍然期望结果是可线性化的，而顺序一致的调度是不可组合的。
顺序一致性与可线性化的主要区别在于没有全局强制的时间界限。在可线性化中，一个操作必须在其挂钟时间范围内变得有效。
顺序一致性放宽了这一要求：操作的结果可以在操作完成后才变得可见，，只要从单个参与者的角度来看顺序是一致的。出现的顺序必须对所有读取者一致。
#### 因果一致性
在这些操作之间建立了因果顺序。即使后一个写入比前一个传播得更快，在它的所有依赖项到达之前，该写入是不可见的，并且事件顺序会从它们的逻辑时间戳重建。
因果一致性可以使用逻辑时钟来实现，在每个消息中发送上下文元数据，并总结哪些操作在逻辑上先于当前操作。当从服务器接受到更新时，更新包含了上下文的最新版本。上下文不匹配的消息将缓存在服务器上。
COPSE通过键版本跟踪依赖关系，，而Eiger则建立操作顺序。它们都不像最终一致的存储那样暴露无序操作，相反，它们检测和处理冲突。
进程维护逻辑时钟的向量，其中每个时钟对应一个进程。每个时钟从初始值开始，每次新事件到达时递增。当从其他进程接受到时钟向量时，进程将其本地向量中的各个值更新为接收到向量中对应进程时钟的最大值。
利用向量时钟，我们可以模拟公共时间和全局状态，将异步事件表示为同步事件。进程维护逻辑时钟的向量，其中每个时钟对应一个进程。当从其他进程接收到时钟向量时，进程将其本地向量中的各个值更新为接收到向量中对应进程时钟的最大值。
为了使用向量时钟来解决冲突，每当我们对数据库进行写入时，首先检查写入键的值是否已经在本地存在。如果存在，我们将一个新版本追加到版本向量中，从而建立两个写入之间的因果关系。
由于我们要找的是一个能够提高可用性和性能的一致性模型，我们必须允许副本出现分歧。如读到过期数据，或者接收潜在冲突的写操作，因此系统允许创建连个独立的事件链。而向量时钟可以告诉你冲突已经发生，但并不提出确切的解决方法，因为解决语义通常是应用程序特定的。因此，一些最终一致的数据库，不会对操作进行因果排序，而使用“最后写胜出”规则来解决冲突。
### 会话模型
在分布式系统中，客户端通常可以连接任何可用的副本，如果最近对一个副本的写入结果没有传播到另一个副本，则客户端可能无法观察到该写入所做的状态更变。
单调读
单调写
读后写，确保写入被排在之前的读取操作观察到的写入之后。
将单调读、单调写和读自己写结合起来，可以提供流水线随机访问存储器一致性，也称为FIFO一致性。
### 最终一致性
在多处理器编程和分布式系统中，同步都是昂贵的。放松一致性保证，如顺序一致性允许读取不同的速度传播。
在最终一致性下，更新将异步在系统中传播。形式上说，它声明如果没有对数据项执行额外的更新，最终所有的访问都会返回最新写入的值。
### 可调一致性
复制因子 N 存储数据副本的节点数
写入一致性 W 写入成功需要确认的节点数
读取一致性 R 使读取成功需要响应的节点数
R + W > N 系统可以保证返回最近写入的值
N = 3 W = 2 R = 2时，系统仅能容忍一个节点故障
在执行写操作时，协调者应该将其提交给N个节点，但是在继续操作之前只等待W个节点，其余节点可以异步完成或失败。
当执行读操作时，协调者必须收集至少R个响应。某些数据库使用推测执行提交额外的读取请求以减少协调者响应延迟。
写入多的系统可以选择 W = 1 R = N
增加读或写一致性级别会增加延迟，并提供可用性的要求
Quorum
(N / 2)下取整 + 1 个节点组成的一致性级别称为Quorum. 
在2f + 1个节点的系统中，如果不可用的节点数不大于f，则活动节点还可以继续接收写入或者读取。系统最多能容忍f个节点故障
在不完全写入的情况下，使用Quorum进行读写不能保证单调性。如果在向三个副本中的一个副本写入值之后，某个写操作失败，则Quorum读取可能返回不完全操作的结果或者旧值，具体取决于所联系的副本。为了实现单调性，我们需要使用阻塞读修复。
### 见证者副本
使用见证者副本降低存储成本。我们不需要在每个副本上存储数据记录的拷贝，而是可以将副本分为拷贝副本和见证者副本。拷贝副本仍然持有以前的数据记录。在正常操作下，见证者副本仅存储一些记录，表示写操作发生过的事实。
而在写入超时或者拷贝副本发生故障的情况下，为了临时存储记录，可以升级见证者副本以临时代替故障或者超时的拷贝副本。一旦原来的拷贝副本恢复，升级的副本进行退回。
更一般地，只要我们遵循如下两个规则，那么n个拷贝副本和m个见证者副本就能达到与n+m个副本相同的可用性保证：
1.使用多数派（N/2 + 1个参与者）执行读和写操作
2.Quorum中至少有一个副本是拷贝副本
### 强最终一致性和CRDT
更新可能会延迟或无序地传播到其他节点，但当所有更新最终传播到目标节点时，它们之间的冲突可以被解决，并且它们合并后将产生相同的有效状态。
CRDT是一种特殊的数据结构。它排除了冲突的存在，允许以任意顺序应用对它的操作而不改变结果。这个特性在分布式系统中非常有用。这样的特性使得CRDT在最终一致性系统中很有用，因为它允许系统中的副本状态临时出现分歧。副本可以在本地执行，而无须事先与其他节点同步，操作最终会（可能无序地）传播到所有其他副本。CRDT让我们能够从局部的个体状态或操作序列重建完整的系统状态。
操作成功传递所依赖的前提条件是：要确保系统已经达到操作可以应用的状态。
通过节点用于递增计数的向量P和节点用于递减计数的向量N，可以实现一个支持增减的正负计数器。在较大规模的系统中，为了避免传播巨大的向量，可以使用超级对等节点。超级对等节点复制计数器的状态，从而帮助避免持续的点对点交流。
要保存和复制值，我们可以使用寄存器。最简单的寄存器是最后写入胜出寄存器，它为每个值存储一个唯一的、全局排序的时间戳以解决冲突。
CRDT的另一个例子是无序仅增长集合。我们可以使用两个集合来支持增加和删除。在这种情况下，我们必须遵循一个约束：只有在增加集合中的值才能被插入到删除集合中。要想重建集合的当前状态，可以从增加集合中移除被包含在删除集合中的所有元素。

## 反熵和传播
快速而可靠的传播可能不太适用于大量数据记录，但是对集群范围的元数据却很重要，例如成员信息、节点状态、故障情况、表结构变更等。这些信息出现频率不高数据量很小，但必须被尽可能快且可靠地传播。
更新传播到集群中地所有节点通常有三大类方法：
1.通知从一个进程广播到其他所有进程
2.定期地点对点交换信息，对等节点成对地连接交换信息
3.合作广播，消息接收者会成为广播者，帮助更快更可靠地传播信息。
在大型集群中，由于节点数量庞大，广播会变得昂贵。并且由于过度依赖单个节点，广播也很不可靠。
在最终一致地系统中，反熵被用来降低收敛地时间界限。为了保持各节点地同步，反熵会触发一个后台或者前台进程，比较和调和丢失或冲突地记录。后台反熵进程会利用Merkle树之类地辅助数据结构，从更新日志中识别出分歧地数据。前台反熵进程会捎带地在读取或写入请求熵附加额外逻辑，比如提示移交、读修复等。
### 读修复
读取时最容易检测出副本之间地差异，我们不会查询每个副本上存储地完整数据集，而是将目标限制在哪些客户端请求地数据。
如果副本地响应不同，协调者会把缺失地变更发送给相应地副本。这被称为读修复。有些Dynamo式地数据库选择不要求联系所有的副本，而改用可调的一致性级别。不需要所有副本，只需要满足一致性级别的节点数。
### 摘要读
协调者可以算出完整读取的哈希值，并将其与所有其他节点返回的摘要进行比较。
### 提示移交
提示移交式写侧修复机制。如果目标节点未能确认写入，则写入协调者或某一副本会存储一条特殊的记录，称为一个提示，当目标节点恢复后，该记录会立即被重放过去。Sloppy Quorum提高了可用性但牺牲了一致性。
### Merkle树
Merkle树是一个对本地数据的紧凑的哈希表示，它是一颗由哈希值构成的树。哈希树的底层是通过扫描整个表的数据记录，较高层级则来自对较低层级的哈希值再次进行哈希，从而建立一个层次结构表示形式。
### 位图版本向量
为了跟踪副本状态，我们使用节点本地逻辑时钟。每个时钟代表一组点的集合，表示该节点直接看到或间接看到的写入。
在节点的逻辑时钟中，节点本身协调的事件之间没有间隙。如果某些写操作没有从其他节点复制过来，时钟则会包含间隙。为了让两个节点重新同步，可以让它们交换逻辑时钟，识别出缺失的点所表示的间隙，然后复制与之相关联的数据记录。我们重构出每个点引用的数据记录。这项信息存放在点因果容器中，它将各点映射到给定键上的因果信息。
与其他节点交换期间，它将接收到其他节点看到过而自己缺少的更新。一旦系统中的所有节点都看到下标i及其之前的所有连续值，就可以将版本向量截断到该下标。
有一个可能的缺点，如果节点长时间宕机，对等节点就一直无法截断日志，因为当滞后节点恢复时，还需要将数据复制给它。
### Gossip传播
持有需要传播的记录的进程成为传染性的，而任何尚未收到更新的进进程称为易传染的。传染性的进程经过一段时间的主动传播之后，不再传播新状态，我们称为已删除。
为了避免显式的协调和维护全局的接收者列表，以及避免要求单个协调者想系统中的每个参与者都广播消息，这类算法使用兴趣损失函数建模完整性。
由于Gossip协议通常不需要显式的协调，因此它们在具有灵活的成员关系（节点频繁加入和离开）或网状网络的系统中很有用。
由于消息是以随机方式中继的，因此即使节点之间的某些通信组件发生故障，消息仍可以通过其他路径传递。可以说这样的系统天然能够适应故障。
#### Gossip技术细节
系统达到收敛所需的时间称为延迟。达到收敛和将消息传递给所有对等节点这两个概念存在细微的差异。因为消息可能会在很短时间内就通知到所有对等节点，但Gossip仍在继续。
一段时间后，随着节点注意到它们一次又一次地接收到相同地消息，消息将开始失去重要性，节点最终将会停止中继。
#### 覆盖网络
尽管Gossip协议很重要也很有用，但它们通常只适用于有限范围地问题。非传染性地方法可以以非概率性地确定性、较少地冗余和往往更优地方式来分发消息。Gossip算法主要优势在其扩展性，以及能在logN各消息回合内分发一条消息。为了达到可靠性，Gossip协议会产生一些重复消息的传递。
两种方法的折中是在Gossip系统中构建一个临时的固定拓扑。为此，我们可以创建对等节点间的覆盖网络。节点可以对它的对等节点进行采样，并根据接近程度选择最佳的联系点。也可以生成树，不含重边的无向无环图。
#### 混合Gossip
每个节点将完整的消息发送给很小的节点子集，而对于其余节点，它只是惰性地转发消息ID。如果节点接收到它从未见过地消息标识符，它可以查询对等节点以获取这条消息。
#### 局部视图
混合局部视图协议维护集群地较小地活跃视图和较大地被动视图。活跃视图中地节点构成一个可用于传播消息地覆盖图。被动视图则用于维护节点列表，用于替换活跃视图中的故障节点。
一段时间后，进行洗牌，将对等节点视图中地成员都添加到自己地被动视图中，并删除旧值以限制列表大小。

## 分布式事务
### 多个操作的原子性
原子提交不允许参与者之间出现分歧，只要又一个参与者反对，事务就不能提交。原子提交算法无法正常工作，这时进程可能会撒谎或者决定一个任意值，而这会破坏全体的共识。
两阶段提交（它解决了提交的问题，但不允许协调者发生故障）和三阶段提交（它解决了非阻塞原子提交问题，即使协调者发生故障时，参与者也可以继续提交）
### 两阶段提交
参与者投票接受或拒绝协调者提议的某个值。通常来说，这个值就是要执行的分布式事务的标识符。
协议本身对协调者角色没有任何限制。出于可靠性或性能的原因，可以将该角色转移给其他参与者。
#### 2PC中的参与者故障
如果其中一个参与者在提议阶段故障，协调者无法继续进行提交，因为它要求所有投票都赞成。如果一个参与者不可用，协调者将中止事务，影响可用性。
如果参与者在接受提案后发生故障，将保留在协调者的决策日志复制给参与者，在此之前，参与者不能处理请求。
如果提议阶段中协调者未收到某个副本的响应，会触发超时并中止事务。
#### 2PC中的协调者故障
协调者已经决定了值，但未能传达给某个副本。可以从其他参与者的事务日志或者备份协调者的日志中找到决策信息
如果协调者在收集投票后，广播投票结果之前发生故障，则参与者最终处于不确定状态。协调者无法继续进行提交或中止操作，这使得集群处于未决状态。因此2PC被称为一种阻塞原子提交算法。如果协调者无法恢复，它的替代者只能再次为给定事务收集投票。
### 三阶段提交
3PC协议增加了一个额外的步骤，并且双发都具有超时机制，使得参与者在协调者发生故障时仍能继续提交或中止。3PC假定同步网络模型，且不存在通信故障。
一旦所有参与者成功进入已准备状态并且协调者收到了它们的准备确认，无论其中任何一方发生故障，事务都将被提交。因为参与者看到的状态是相同的。
3PC中的协调者故障
3PC的最坏场景是网络分区。一些节点已成功进入准备状态，在超时后将会提交。有些节点无法与协调者通信，在超时后中止。者导致了脑裂，处于不一致且矛盾的状态。
虽然3PC解决了2PC阻塞问题，但带来了更大的消息开销与潜在的不一致性。这就是3PC不被广泛使用的原因。
### Calvin分布式事务
还有其他方法可以减少争用并减少事务持有锁的总时间。一种方法是让各副本在获取锁并继续执行之前，就执行顺序和事务边界达成一致。这样，节点故障就不会导致事务中止，因为节点可以从并行执行同一事务的其他参与者中恢复状态。
传统数据库使用两阶段锁或乐观并发控制来执行事务，并没有确定的事务顺序。这意味着必须协调各节点以保证事务顺序。而确定性事务顺序消除了执行阶段的协调开销，因为所有副本获得相同输入，就有相同输出。这被称为Calvin，一种快速的分布式事务协议。
为了获得确定性顺序，Calvin使用定序器，它是所有事务的入口点。定序器决定事务的执行顺序，并建立全局事务输入序列。为了最大限度地减少竞争和批量决策，Calvin将时间线切成epoch。
当一个事务批处理被成功复制之后，定序器会将其转发给调度器，它负责编排事务地执行。
Calvin中的每个事务具有读取集和写入集。Calvin本身不支持事务依赖于额外的读取来决定读取集和写入集。
执行过程有4步
1. 读取集确定节点本地数据记录
2. 收集事务所需的本地数据
3. 如果当前工作线程正在一个活跃参与者节点上执行，将收到其他参与者发来的数据记录
4. 执行一批事务，将结果持久化到本地存储中。不用将执行结果转发到其他节点，因为它们接受相同的事务输入。
Calvin使用Paxos共识算法或异步复制，有领导者的恢复成本。
### Spanner分布式事务
Calvin通过在定序器上达成共识来建立全局事务执行顺序，Spanner则在每个分区的共识组上使用两阶段提交。
为了实现一致性并建立事务的顺序关系，Spanner使用TrueTime，高精度物理时钟，同时也暴露时钟误差的范围，允许本地操作中引入人为的减速以等待时钟误差范围过去。
Spanner提供三种事务类型：读写事务、只读事务、快照读。读写事务使用悲观锁的并发控制，存在领导者副本。只读事务是无锁的，可以在任意副本上执行。只有在读取最新时间戳时才需要领导者，领导者负责从Paxos组中获取最后提交的值。快照内容一旦写入就无法更改，并会分配一个时间戳。
副本被分组为副本集，称为Paxos组，这是数据放置和复制的单元。在处理跨分片的事务时，领导者会相互通信。
2PC算法要求所有参与者都存活才能成功提交，Spanner使用Paxos组代替单个节点作为参与者，解决了可用性问题。
Paxos组用于多个节点之间一致地复制事务管理器的状态。领导者首先获取锁，并选择一个写入时间戳（比之前任何的时间戳要大），记录一条2PC的prepare日志。事务协调者收集时间戳，生成一条大于任何prepare时间戳的commit时间戳，通过Paxos记录一条commit日志。然后，事务协调者需要等待知道提交时间戳过后，这样保证客户端只能看到时间戳已经过去的事务结果。将其发给客户端和各个领导者，领导者将一条commit日志连同新的时间戳一同记录到所有的Paxos组中，并释放锁。
Spanner读写事务提供了称为外部一致性的序列化顺序：事务时间戳反映了序列化顺序。
### 数据库分区
数据分区最直接的方法是将数据划分成多个范围，并允许每个副本集只管理特定的范围。执行查询时，客户端需要基于 路由键将读写请求路由到正确的副本集。这种分区方案通常被称为分片（sharding）。
将读写负担较重的范围分裂成更小的分区，从而分散负载。
当集群添加或删除节点时，数据库必须重新分区数据以保持均衡。
一致性哈希
路由键的值被输入哈希函数，返回的哈希值被映射到一个环上，以便在超过最大值之后回卷到最小值。
### Percolator分布式事务
由于允许一些读取和写入异常，隔离级别可能很难论证。如果应用程序不要求可串行化，可以使用快照隔离事务模型。
如果存在写-写冲突，只有一个提交能成功，这种策略称为首个提交者胜利。
快照隔离能避免读偏斜，快照隔离仅会让小于某个特定时间戳的值对事务可见。
快照隔离下的操作历史不是可串行化的。由于只有对相同单元格的冲突写入会被中止，因此仍可能发生写偏斜。写偏斜发生在两个事务修改的值集合不相交时，每个事务本身的写入都不违反约束。两个事务都能提交，但两个事务的写入合在一起就会违反这些约束。
快照隔离优势在于读的效率比较高。
Percolator用不同的列保存数据记录、已提交的数据点位置和锁信息。为了避免竞争条件，它使用Bigtable提供的条件修改API，该API允许在单个远程调用中执行读-修改-写操作。
每个事务必须和授时节点通信两次，一次获取事务开始时间戳，另一次提交过程中。写入会被缓存起来，最后由客户端驱动进行两阶段提交。
第一阶段称为预写（prewrite）事务尝试为所有写入的单元格加锁。其中一个锁被标记为主锁，用于客户端恢复。如果检测到有更晚的时间戳写入或者未释放的锁，则中止事务。
第二阶段，客户端开始释放锁，首先释放主锁，用写记录代替锁，让写入对外可见，并更新写入元数据为最新数据点的时间戳。
主锁释放，则事务内容必须提交。不存在两个事务都在尝试修改内容的情况。
### 协调避免
只要操作满足约束融合性，协调是可以避免的。两个满足约束但存在分歧的数据库状态一定能够合并为一个有效的最终状态。这里说的约束对应ACID中的一致性。
允许协调避免的系统模型必须保证以下性质： 全局有效性（所需的约束条件始终是满足的，事务不会观察到无效的状态）、可用性(如果所有包含状态的节点对客户端都是可达的，那么事务必然会成功提交)、收敛(节点可以独立维护其本地状态，如果之后没有新的事务并且网络分区不会无限持续下去)、不需协调（本地事务独立于其他节点操作）
一个实现协调避免的例子是读原子性多分区(RAMP) 提供以下两个性质：同步独立性、分区独立性。
事务更新要么全部对并发事务可见，要么全部不可见。提供原子的写可见性，不需要互斥，这意味着事务不会相互阻塞。
当某个分区中的写入变为可见时，此事务中所有其他分区的写入也必须可见（解决全局范围内生效）
RAMP使用两阶段提交来处理写入
1. 准备阶段 写操作放入相应目标分区，对外不可见
2. 提交/中止阶段 将事务的写操作进行的状态变为可见
RAMP允许一个记录同时存在多个版本，为了正在进行的读取请求。
## 共识
FLP不可能定理表明，对于完全异步系统，不可能确保在有限时间达成共识。即使消息传递是可靠的，进程也无法得知另一进程是奔溃了还是运行缓慢。
外部故障检测器提供有关其他进程的信息确保算法的活动性，但并非完全准确，因此可能会出现这样的情况：故障检测器错误地怀疑某个进程有故障而导致算法重启。
共识算法有三个性质:一致性、有效性(决定的值是由其中一个进程提议的)、终止性
### 广播
有许多种广播，它们基于不同的假设，提供不同的保证。
广播信息最简单直接的方法是尽力而为广播。如果失败，其他参与者不会尝试重新广播该消息；如果协调者奔溃，广播将静默地失败。
最简单地回退机制是允许每个接受信息地进程将消息转发给它知道地所有其他进程。用N^2条信息洪泛整个网络。
### 原子广播
洪泛能保证消息送达，但不能保证送达顺序。要保证顺序就需要使用原子广播也被称为全序多播，它能保证可靠传递和全序性。
#### 虚同步
原子广播将完全有序地消息传递给一组静态地进程，而虚同步则将完全有序地消息传递给一组动态地进程。
虚同步将进程分成组。只要组存在，消息就会以相同地顺序传递给组内所有成员。模型本身并未指定顺序，只要它提供地顺序对所有成员一致即可，某些实现利用这一点来提高性能。
各进程具有相同地组视图，每条消息都关联了一个组标识，只要是同一个组地进程，就能看到相同的消息。
组视图变更就像是消息广播无法通过的屏障。
一些全序广播算法使用定序器对消息进行排序。
尽管技术上很可靠，但虚同步并未被广泛采用，在商业系统中很罕见。
#### Zookeeper原子广播
ZAB中进程有领导者和跟随者，领导者是一个临时角色，负责驱动整个算法流程、广播消息给跟随者并建立事件的顺序。当写入新记录或读取最新值时，客户端连接到集群的某一节点，如果该节点恰好时领导者，它将处理该请求，否则请求将被转发给领导者。
为了保证领导者的唯一性，协议时间线被划分成epoch，通过唯一且单调递增的序列号标识。每个epoch中只能有一个领导者。
一旦确定了潜在领导者，该算法分为三个阶段执行协议：
1.发现阶段 潜在领导者了解其他所有进程已知的最新epoch并提出一个新的epoch，该epoch大于任意跟随者的当前epoch。跟醉者收到该协议后，回复前一个epoch中看到过的最新事务标识符。这一步之后，进程将不再接受更早epoch的广播提议。
2.同步阶段 此阶段用于从前领导者的故障中恢复，并让滞后的追随者更快地追上进度。同步阶段中，新领导者会确保各个跟随者拥有相同的历史记录，并将先前epoch领导者已提交的提案转发给跟随者。当这些提案传递完之后才会开始发送新epoch的提案。
3.广播阶段 领导者接收客户端的消息，确定消息顺序，并广播给跟随者。领导者发出一个新的提议，等待Quorum的跟随者确认，最后提交。
如果领导者未从Quorum的跟随者那里接受到心跳，它将不再担任领导者并重新发起选举。同样地，如果某个跟随者认为领导者已经崩溃，它将发起新的选举。
消息是完全有序的，在上一条消息确认之前，领导者不会尝试发送下一条消息。消息完全有序也提高了ZAB的恢复效率。在同步阶段，跟随者回复序列号最高的已提交提案。领导者可以简单地选择具有最高提案的节点用于恢复，这可以是唯一的需要从中复制消息的节点。
ZAB的一个优点是效率高，广播过程仅需两轮消息，而且领导者故障只需从一个最新的进程中获取缺失消息就可以恢复。
### Paxos
提议者 从客户端接收值，创建提案，尝试从接受者收集投票
接受者 投票接受或拒绝提议者提议的值。为了容错，算法要求存在多个接受者，但为了算法活动性，只要Quorum的接受者投票。
学习者 副本，保持被接受者提案的结果。
任何参与者都可以扮演任何角色，大多数实现将多个角色放在一起。
每个提案包含一个由客户端提出的值和一个唯一且单调递增的提案编号，该编号之后还会被用于确保操作的全序性。提案编号通常用（id，timestamp）实现，其中节点ID是可比较的，可用于防止时间戳冲突。
#### Paxos算法
算法分为两个阶段：投票（提议）阶段和复制阶段。在投票阶段，提议者竞争领导权；复制阶段，提议者将值分发给接受者。
在提议阶段中，提议者发送一条Prepare（n）消息（其中n是提议编号）给多数派接受者，并收集它们的投票。
接受者收到准备请求后需要按以下要求做出回应
1. 如果该接受者尚未回应过编号更高的准备请求，则它承诺不再接受任何编号更低的提案
2. 如果该接受者已经接受了（Accpt！（m，v）消息）任何其他提案，它将回复一条Promise（m，v）消息，通知提议者它已经接受了编号为m的提案。
3. 如果该接受者已经回应过更高编号的准备请求，则通知提议者存在编号更高的提案
4. 接受者可以回应多个准备请求，只要后来接收到的请求具有更高的编号。
复制阶段，当收集到多数票之后，提议者可以开始复制过程：提议者向接受者发送一条Accept消息以提交这个提案，其中v为值，n为提案编号。v是在接受放收到的回复编号最高的提案中的值，如果所有回复均不包含旧的、已接受的提案，那么v可以是提议者自己提出的任何值。
#### Paxos的Quorum
Quorum背后的主要思想是：即使参与者发生故障或恰好被网络分区隔开，也至少有一个参与者可以充当仲裁者，以确保协议正确性。
Paxos可以保证在无论多少节点发生故障时都具有安全性。任何配置都不会产生不正确或不一致的状态，不然就违背了共识的定义。
Quorum仅描述了系统的阻塞属性。为了保证安全性，每一步中我们都要等待至少Quorum个节点的响应。我们可以发送Propose和Accept！消息给更多节点，但不必等待它们的回应。一些系统使用推测执行：发出冗余查询。但为了保证活动性，我们可以在收到Quorum的响应后立即继续。
由于算法状态已被复制到多个节点，提议者故障不会导致无法达成共识。但凡提议者宕机之前有一个参与者接受了提案的值，该提案都能被下一个提议者重新拾取。这也意味着，所有这些都可以在原始提议者不知道的情况下发生。
尽管接受者承诺不接受任何编号更小的提案，但只要后来的提案编号更大，它们仍有可能回复多个准备请求。当提议者尝试提交该值时，它可能会发现接受者已经回复了编号更大的准备请求。这可能导致多个提议者不断重试并一直阻止彼此继续。我们通常引入随机退避机制来解决该问题，最终，一个提议者将继续执行，而另一个提议者将等待。
Paxos算法能容忍接受者故障，但剩余的接受者必须足以构成多数派。
#### Multi-Paxos
经典的Paxos算法存在的一个问题是，系统中的每轮复制都需要进行一轮提议。只有当选出提议者之后才会进入复制阶段。为了避免重复进行提议阶段，使提议者可以重用已被认可的地位，我们可以使用Multi-Paxos。Multi-Paxos引入了领导者的概念————提议者的负责人。这能显著提高算法的效率。
经典Paxos算法中，读取可以通过执行一轮Paxos回合实现，它从不完整的回合中收集值。之所以必须这么做，是因为最后一个已知的提议者无法保证一定包含最新的数据，可能另一个提议者已经修改了状态，而当前提议者并不知道。
Multi-Paxos中也可能发生类似的情况，为了避免这种情况并确保其他进程都无法成功地提交值，一些Multi-Paxos实现使用租约机制。领导者定期与参与者通信，通知它们自己还活着，同时也延长了租约。参与者需要做出回应，允许领导者继续在位，并承诺在租约期内它们不会接受其他领导者的提案。
租约不是正确性保证，而是一种性能优化，它允许直接从当前领导者读取数据而不用收集Quorum个回应。为了确保安全性，租约依赖于参与者之间有界的时钟同步。如果时钟偏移太大，这种情况下无法保证可线性化。
Multi-Paxos有时被描述为应用在某数据结构上的操作的复制日志。当日志与主数据结构同步后，创建一个快照，之后就可以截断日志。
#### 快速Paxos
通过允许任何提议者直接联系接受者而不是通过领导者，我们可以在经典Paxos算法的基础上减少一次网络交互。为此，我们需要将Quorum从原来的f + 1增加到2f + 1，并将接受者的总数增加到3f + 1，这种优化被称为快速Paxos。
在快速回合汇总，如果允许协调者在复制阶段选择自己的值，那么它可以向接受者发出一条特殊的Any消息。这时，接受者可以接受任何提议者的值，就像经典回合中从协调者那里收到带有该值的消息一样。换句话说，各接受者独立地决定它们从不同提议者那里获得的值。
如果两个或多个提议值都在尝试用快速步骤减少网络交互次数，接受者将收到多个不同的值，就会发生冲突。协调者必须进行干涉，发起新的回合来恢复。
快速Paxos的一个缺点是：如果请求速率越高，冲突会导致网络交互次数更多、请求延迟更长。由于副本数量的增加，以及随之而来的参与者之间交换的消息增加，尽管步骤减少了，但快速Paxos的延迟可能比经典Paxos更长。
#### 平等Paxos
用提议者负责人作为领导者会让系统更容易发生故障：领导者一旦发生故障，需要选出一个新的领导者，算法才能继续执行。另一个问题是，领导者的工作负载比其他提议者更大，从而影响系统性能。
一种避免让领导者承担整个系统负载的方法是分区。这一举措有助于提高可用性、性能和可扩展性。
不同于Multi-Paxos使用领导者和提案编号来对命令进行排序，我们可以为每个特定命令使用一个领导者，通过查找并建立依赖关系来对命令进行排序。这种方法通常称为平等Paxos。允许无冲突写入独立地提交到复制状态机的这个想法最早在文献中引入，称为广义Paxos。
经典Paxos提高可用性，因为每个回合都会选出一个领导者，但是消息复杂度更高。Multi-Paxos提供高吞吐量，消息数量更少，但是领导者可能称为瓶颈。
收集这些信息后，它发送一条Pre-Accept消息给快速Quorum的副本。快速Quorum包含（3f/4）上取整个副本，其中f是可容忍的故障数。
副本检查本地命令日志，在本地视图中查找可能冲突的提案，并依次更新提案的依赖关系，然后将这些信息回复给领导者。如果领导者从快速Quorum的副本收到响应，并且发现各副本以及领导者的依赖项列表均一致，则可以提交这个命令。
如果领导者没有收到足够的响应，或者从副本接收到的依赖命令列表不同且包含有干扰的命令，它将用新的依赖项列表以及序列号更新当前提案。新的序列号必须大于各副本中看到的最高序列号。领导者将新的命令发送给（f / 2）下取整 + 1 个副本。
快速路径：当依赖项匹配，领导者可以安全提交，只需要快速Quorum的副本即可。
慢速路径：副本存在分歧，必须更新各副本的命令列表，然后领导者才能继续提交。
由于在预接受阶段已经收集了依赖关系，因此在执行请求时，命令间的顺序已经确定，命令之间不会突然冒出别的命令，新命令只会追加到序列号最大的命令之后。
副本在执行一条命令之前，必须先执行它所有的依赖项。考虑到只有相互干扰的命令才存在依赖，因此这种情况对于大多数工作负载来说应该相对罕见。
#### 柔性Paxos
无论我们如何选择节点，两个Quorum间必然存在交集，总是至少存在一个节点将它们联系在一起。
如果我们定义参与者总数为N，提议阶段所需节点数为Q1，接受阶段所需节点数为Q2，只要保证 Q1+ Q2 > N 即可。由于第二阶段比第一阶段更常见，要想让Q2包含 N / 2 个接受者，只要将Q1调得更大，相应的算法被称为柔性Paxos。
只要当前的领导者稳定，就可以持续进行复制阶段并容忍最多N - Q2个节点故障。
另一个用到相交Quorum思想的事Paxos变体是垂直paxos。区分读取和写入Quorum，二者必须相交，领导者需要为一个或多个编号较小的提案收集较小的读Quorum，并为自己的提案收集较大的写入Quorum。
### Raft
候选者： 领导者的位置时暂时的，任何参与者都可以担任此角色。要成为领导者，节点必须先变成候选者，并尝试收集多数派的投票。
领导者： 负责处理客户端请求并复制状态机进行交互。领导者当选的时间段称为任期。每个任期用一个单调递增的数字来标识，它可以持续任意长的时间段。
跟随着： 被动参与者，负责保存日志条目以及响应领导者和候选者的请求。
有时候，不同参与者眼中的当前任期可能不一致，因为它们可能在不同的时刻得知新任期，或者可能错过了一个或者多个任期的领导者选举。每条消息都包含一个任期标识符，如果参与者发现自己的任期已过期，则将其更新为编号较大的那个。这意味着，在任一时刻可能存在多个任期，但当发生冲突时，编号较高的任期获胜。只有发起新的选举或发现当前任期已过期时，节点才会更新它的任期。
Raft用随机定时器来降低再次平票的概率。其中一个候选者可以略早发起下一轮选举，从而收集到足够的票数，而其他人则等待并做出让步。这种方法可以在不引入其他协调的情况下加速选举速度。
如果未能在预期时间内收到确认，它可以尝试再发送消息。作为一种性能优化的方式，它也可以并行发送多个消息。
新当选的领导者必须将集群状态恢复到最新的日志条目。为此，它先找到共同基础点(领导者和跟随者都认同的最后日志条目)，并命令=跟随者丢弃在此之后的所有（未提交的）条目。然后，它将自己日志中的最近条目发送给跟随者，覆盖其历史记录。领导者自己的日志记录永远不会被删除或覆盖，只会将条目追加上去。
### 拜占庭共识
有时分布式系统也会部署在有潜在对抗性的环境中，其中各节点并非由同一实体控制，我们需要用算法来保证：即使在某些节点行为失常甚至带有恶意的情况下，系统仍然能够正常运行。除了人为的恶意之外，拜占庭式故障还可能由bug、错误配置、硬件问题或数据损坏引起。
大多数拜占庭共识算法需要用N^2条消息完成一个算法步骤，其中N表示Quorum的大小，因为Quorum中的每个节点都必须与其他所有节点通信。每个步骤都需要用其他节点做交叉验证，这么做是必需的，因为节点不能彼此信赖，也不能依赖领导者，必须将返回的结果与多数响应进行比较才能验证其他节点的行为。
使用拜占庭容错（PBFT）假设节点故障相互独立。系统具有弱同步假设，就像你期望中的正常网络那样：故障可能会发生，但不会无限持续下去，最终一定会恢复。
节点间的所有通信都经过加密，以防止伪造信息或网络攻击。副本知道彼此的公钥，通过它来验证身份并加密信息。缺陷节点可能会泄露系统内部的信息，因为即使用了加密，每个节点也需要解密信息内容并做出回应。这不会危害到算法，因为它有不同的用途。
#### PBFT算法
为了保证PBFT算法的安全性和活动性，缺陷节点不能超过 （n - 1） / 3 个。系统若要容忍f个缺陷节点，则需要至少n = 3f + 1个节点。之所以如此，是因为需要多数派节点同意一个值：可能存在f个缺陷副本，还可能有f个副本没有响应但并非因为缺陷。算法必须能够从无缺陷的副本中收集到足够的响应，以保证仍能超过缺陷副本的响应数量。
PBFT中共识的性质与其他共识算法相似：尽管可能发生故障，但所有无缺陷的副本必须就接收到的值的集合及顺序达成一致。
为了区分集群的配置，PBFT使用视图。每个视图中有一个副本是主副本，其余副本被视为备份副本。所有节点都被连续编号，主节点的序号是v mod N，其中v是视图ID，N是当前配置下的节点数。当主节点发生故障时，视图也会发生变化。客户端通过这节点执行操作，主节点将请求广播到备份节点，备份节点执行请求并将响应发回客户端。客户端等待 f + 1个副本回复相同的结果，才能确认执行成功。
主副本收到客户端请求后，协议执行三个阶段
1. 预准备： 主节点广播一条消息，其中包含视图ID、一个唯一的单调递增标识符、载荷和载荷的摘要。如果备份节点的视图与主节点的视图匹配，且请求未被篡改，那么备份节点会接受这条信息。
2. 准备： 向其他所有副本广播Prepare信息，其中包含视图ID、信息ID和载荷的摘要。只有当接收到 2f 个来自不同备份节点的Prepare信息并且这些消息的视图、ID和摘要均相同时，节点才能进入下个阶段。
3. 提交： 将Commit信息广播到其他所有副本，并等待收集到 2f + 1 条匹配的Commit信息。
2f这个数字很重要，因为算法需要至少 f + 1 个无缺陷副本对客户端做出响应。
当副本收集到足够多的提交信息时，它们会通知客户端并完成当前回合。只有当收到 f + 1 个匹配的响应后，客户端才能确定执行已正确完成。
当副本注意到主节点处于非活动状态且怀疑它可能发生了故障时，就会发生视图变更，并广播一个视图变更通知等待确认。当新视图的主节点收到 2f 个视图变更事件时，它将发起一个新视图。
PBFT中的只读操作仅需一轮网络交互即可完成。客户端向所有副本发送读请求。各副本等到所有进行中的修改该值的操作完成提交，然后在其试探性状态上执行请求并回复客户端。当从不同副本收集到值相同的 2f + 1个响应后，操作就完成了。
#### 恢复和检查点
副本将接受的信息保存在一个稳定的日志中。每条消息至少要保留到 f + 1 个节点执行过为止。
为了证明这些状态是正确的，节点计算出到某个给定序列号为止的信息的状态。
每隔N个请求，主节点会创建一个稳定检查点：它广播一个最新的序列号以及相应状态的摘要。然后，它等待 2f + 1 个副本响应，这些响应构成了该检查点的证明，保证各副本可以安全地丢弃给定序列号之前地所有预准备、准备、提交和检查点信息。
理解拜占庭容错性很重要，在系统各部分之间没有信任时，必须采用类似PBFT的算法。容忍拜占庭故障的算法会带来显著的信息交换开销。
### 本章小结
Multi-Paxos 允许提交者保留这一角色，提议多个值而不是一个。
Raft将日志复制、领导者选举和安全性三个方面分开。
为了保证对抗性环境中的共识安全性，应当使用拜占庭容错算法。