# 数据库
## 基础
### 数据库基础知识总结
#### drop、delete 与 truncate 区别？
##### 用法不同
- drop(丢弃数据): drop table 表名 ，直接将表都删除掉，在删除表的时候使用。
- truncate (清空数据) : truncate table 表名 ，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用。
- delete（删除数据） : delete from 表名 where 列名=值，删除某一行的数据，如果不加 where 子句和truncate table 表名作用类似。
###### 属于不同的数据库语言
truncate 和 drop 属于 DDL(数据定义语言)语句，操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发 trigger。而 delete 语句是 DML (数据库操作语言)语句，这个操作会放到 rollback segment 中，事务提交之后才生效。
### NoSQL基础知识总结
#### NoSQL 数据库有哪些类型？
- 键值：键值数据库是一种较简单的数据库，其中每个项目都包含键和值。这是极为灵活的 NoSQL 数据库类型，因为应用可以完全控制 value 字段中存储的内容，没有任何限制。Redis 和 DynanoDB 是两款非常流行的键值数据库。
- 文档：文档数据库中的数据被存储在类似于 JSON（JavaScript 对象表示法）对象的文档中，非常清晰直观。每个文档包含成对的字段和值。这些值通常可以是各种类型，包括字符串、数字、布尔值、数组或对象等，并且它们的结构通常与开发者在代码中使用的对象保持一致。MongoDB 就是一款非常流行的文档数据库。
- 图形：图形数据库旨在轻松构建和运行与高度连接的数据集一起使用的应用程序。图形数据库的典型使用案例包括社交网络、推荐引擎、欺诈检测和知识图形。Neo4j 和 Giraph 是两款非常流行的图形数据库。
- 宽列：宽列存储数据库非常适合需要存储大量的数据。Cassandra 和 HBase 是两款非常流行的宽列存储数据库。
### 字符集详解
#### 有哪些常见的字符集？
- ASCII
- GB2312 GBK GB18030 对于英语字符，GB2312 编码和 ASCII 码是相同的，1 字节编码即可。对于非英字符，需要 2 字节编码。GB2312 字符集不支持绝大部分的生僻字和繁体字。GB18030 完全兼容 GB2312 和 GBK 字符集，纳入中国国内少数民族的文字，且收录了日韩汉字，是目前为止最全面的汉字字符集，共收录汉字 70000 多个。
- BIG5 BIG5 主要针对的是繁体中文
- Unicode & UTF-8 UTF-8 使用 1 到 4 个字节为每个字符编码， UTF-16 使用 2 或 4 个字节为每个字符编码，UTF-32 固定位 4 个字节为每个字符编码。
#### MySQL 字符集
##### 字符集的层次级别
MySQL 中的字符集有以下的层次级别：
- server（MySQL 实例级别）
- database（库级别）
- table（表级别）
- column（字段级别）
##### 连接字符集
- character_set_client ：描述了客户端发送给服务器的 SQL 语句使用的是什么字符集。
- character_set_connection ：描述了服务器接收到 SQL 语句时使用什么字符集进行翻译。
- character_set_results ：描述了服务器返回给客户端的结果使用的是什么字符集。
##### UTF-8 使用
- utf8：utf8编码只支持1-3个字节 。 在 utf8 编码中，中文是占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节。
- utf8mb4：UTF-8 的完整实现，正版！最多支持使用 4 个字节表示字符，因此，可以用来存储 emoji 符号。
### SQL
#### SQL常见面试题总结（2）
##### 多条一次性插入 ：INSERT INTO table_name (column1, column2, ...) VALUES (value1_1, value1_2, ...), (value2_1, value2_2, ...), ...
##### 从另一个表导入 ：INSERT INTO table_name SELECT * FROM table_name2 [WHERE key=value]
#####     带更新的插入 ：REPLACE INTO table_name VALUES (value1, value2, ...)（注意这种原理是检测到主键或唯一性索引键重复就删除原记录后重新插入）
###### 未完成试卷数大于 1 的有效用户（较难）
```
SELECT a.uid,
       SUM(CASE
               WHEN a.submit_time IS NULL THEN 1
           END) AS incomplete_cnt,
       SUM(CASE
               WHEN a.submit_time IS NOT NULL THEN 1
           END) AS complete_cnt,
       GROUP_CONCAT(DISTINCT CONCAT(DATE_FORMAT(a.start_time, '%Y-%m-%d'), ':', b.tag)
                    ORDER BY start_time SEPARATOR ";") AS detail
FROM exam_record a
LEFT JOIN examination_info b ON a.exam_id = b.exam_id
WHERE YEAR (a.start_time)= 2021
GROUP BY a.uid
HAVING incomplete_cnt > 1
AND complete_cnt >= 1
AND incomplete_cnt < 5
ORDER BY incomplete_cnt DESC
```
#### SQL常见面试题总结（5）
##### 空值处理
```
SELECT exam_id,
       count(submit_time IS NULL OR NULL) incomplete_cnt,
       ROUND(count(submit_time IS NULL OR NULL) / count(*), 3) complete_rate
FROM exam_record
GROUP BY exam_id
HAVING incomplete_cnt <> 0
```
## MySQL
### MySQL常见面试题总结
#### MySQL 字段类型
##### CHAR 和 VARCHAR 的区别是什么？
CHAR 在存储时会在右边填充空格以达到指定的长度，检索时会去掉空格；VARCHAR 在存储时需要使用 1 或 2 个额外字节记录字符串的长度，检索时不需要处理。
##### VARCHAR(100)和 VARCHAR(10)的区别是什么？
VARCHAR(100) 会消耗更多的内存。这是因为 VARCHAR 类型在内存中操作时，通常会分配固定大小的内存块来保存值，即使用字符类型中定义的长度。例如在进行排序的时候，VARCHAR(100)是按照 100 这个长度来进行的，也就会消耗更多内存。
##### 为什么不推荐使用 TEXT 和 BLOB？
- 不能有默认值
- 在使用临时表时无法使用内存临时表，只能在磁盘上创建临时表
- 检索效率较低
- 不能直接创建索引，需要指定前缀长度
- 可能会消耗大量的网络和 IO 带宽
- 可能导致表上的 DML 操作变慢
##### DATETIME 和 TIMESTAMP 的区别是什么？
DATETIME 类型没有时区信息，TIMESTAMP 和时区有关。
TIMESTAMP 只需要使用 4 个字节的存储空间，但是 DATETIME 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。
##### NULL 和 '' 的区别是什么？
- NULL 代表一个不确定的值,就算是两个 NULL,它俩也不一定相等。例如，SELECT NULL=NULL的结果为 false，但是在我们使用DISTINCT,GROUP BY,ORDER BY时,NULL又被认为是相等的。
- ''的长度是 0，是不占用空间的，而NULL 是需要占用空间的。
- NULL 会影响聚合函数的结果。例如，SUM、AVG、MIN、MAX 等聚合函数会忽略 NULL 值。 COUNT 的处理方式取决于参数的类型。如果参数是 *(COUNT(*))，则会统计所有的记录数，包括 NULL 值；如果参数是某个字段名(COUNT(列名))，则会忽略 NULL 值，只统计非空值的个数。
- 查询 NULL 值时，必须使用 IS NULL 或 IS NOT NULLl 来判断，而不能使用 =、!=、 <、> 之类的比较运算符。而''是可以使用这些比较运算符的。
#### MySQL 存储引擎
##### MySQL 存储引擎架构了解吗？
存储引擎是基于表的，而不是数据库。
##### MyISAM 和 InnoDB 有什么区别？
- 是否支持行级锁 MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。
- 是否支持事务 MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别，具有提交(commit)和回滚(rollback)事务的能力。并且，InnoDB 默认使用的 REPEATABLE-READ（可重读）隔离级别是可以解决幻读问题发生的（基于 MVCC 和 Next-Key Lock）。
- 是否支持外键 MyISAM 不支持，而 InnoDB 支持。
- 是否支持数据库异常崩溃后的安全恢复 MyISAM 不支持，而 InnoDB 支持。
- 是否支持 MVCC MyISAM 不支持，而 InnoDB 支持。MyISAM 连行级锁都不支持。MVCC 可以看作是行级锁的一个升级，可以有效减少加锁操作，提高性能。
- 索引实现不一样 MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是两者的实现方式不太一样。InnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。
- 性能有差别 InnoDB 的性能比 MyISAM 更强大，不管是在读写混合模式下还是只读模式下，随着 CPU 核数的增加，InnoDB 的读写能力呈线性增长。MyISAM 因为读写不能并发，它的处理能力跟核数没关系。
- 数据缓存策略和机制实现不同 InnoDB 使用缓冲池（Buffer Pool）缓存数据页和索引页，MyISAM 使用键缓存（Key Cache）仅缓存索引页而不缓存数据页。
#### MySQL 查询缓存
MySQL 8.0 版本后移除。缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。
查询缓存不命中的情况
- 任何两个查询在任何字符上的不同都会导致缓存不命中
- 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存
- 缓存建立之后，MySQL 的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效
#### MySQL 日志
// TODO MISS
#### MySQL 事务
只有保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。也就是说 A、I、D 是手段，C 是目的
原子性，隔离性和持久性是数据库的属性，而一致性（在 ACID 意义上）是应用程序的属性。应用可能依赖数据库的原子性和隔离属性来实现一致性，但这并不仅取决于数据库。
- 脏读（Dirty read）
一个事务读取数据并且对数据进行了修改，这个修改对其他事务来说是可见的，即使当前事务没有提交。这时另外一个事务读取了这个还未提交的数据，但第一个事务突然回滚，导致数据并没有被提交到数据库，那第二个事务读取到的就是脏数据，这也就是脏读的由来。
- 丢失修改（Lost to modify）
在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。
- 不可重复读（Unrepeatable read）
指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- 幻读（Phantom read）
幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。
##### 并发事务的控制方式有哪些？
锁 和 MVCC。锁可以看作是悲观控制的模式，多版本并发控制（MVCC，Multiversion concurrency control）可以看作是乐观控制的模式。
MVCC 是多版本并发控制方法，即对一份数据会存储多个版本，通过事务的可见性来保证事务能看到自己应该看到的版本。通常会有一个全局的版本分配器来为每一行数据设置版本号，版本号是唯一的。
MVCC 在 MySQL 中实现所依赖的手段主要是: 隐藏字段、read view、undo log。
- undo log : undo log 用于记录某行数据的多个版本的数据。
- read view 和 隐藏字段 : 用来判断当前版本数据的可见性。
##### SQL 标准定义了哪些事务隔离级别?
- READ-UNCOMMITTED(读取未提交) ：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
- READ-COMMITTED(读取已提交) ：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
- REPEATABLE-READ(可重复读) ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
- SERIALIZABLE(可串行化) ：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。
##### MySQL 的隔离级别是基于锁实现的吗？
SERIALIZABLE 隔离级别是通过锁来实现的，READ-COMMITTED 和 REPEATABLE-READ 隔离级别是基于 MVCC 实现的。REPEATABLE-READ 在当前读情况下需要使用加锁读来保证不会出现幻读。
##### InnoDB 有哪几类行锁？
- 记录锁（Record Lock）：也被称为记录锁，属于单个行记录上的锁
- 间隙锁（Gap Lock）：锁定一个范围，不包括记录本身
- 临键锁（Next-Key Lock）：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题（MySQL 事务部分提到过）。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁。
在 InnoDB 默认的隔离级别 REPEATABLE-READ 下，行锁默认使用的是 Next-Key Lock。但是，如果操作的索引是唯一索引或主键，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围。
##### 意向锁有什么作用？
我们需要用到一个叫做意向锁的东东来快速判断是否可以对某个表使用表锁。
意向锁是由数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。
##### 当前读和快照读有什么区别？
1. 快照读（一致性非锁定读）就是单纯的 SELECT 语句。快照即记录的历史版本，每行记录可能存在多个历史版本（多版本技术）。
快照读的情况下，如果读取的记录正在执行 UPDATE/DELETE 操作，读取操作不会因此去等待记录上 X 锁的释放，而是会去读取行的一个快照。
只有在事务隔离级别 RC(读取已提交) 和 RR（可重读）下，InnoDB 才会使用一致性非锁定读
- 在 RC 级别下，对于快照数据，一致性非锁定读总是读取被锁定行的最新一份快照数据
- 在 RR 级别下，对于快照数据，一致性非锁定读总是读取本事务开始时的行数据版本
2. 当前读 （一致性锁定读）就是给行记录加 X 锁或 S 锁。
##### 自增锁有了解吗？
AUTO_INCREMENT的列都会涉及到自增锁。如果一个事务正在插入数据到有自增列的表时，会先获取自增锁，拿不到就可能会被阻塞住。这里的阻塞行为只是自增锁行为的其中一种。innodb_autoinc_lock_mode如下：
- 传统模式
- 连续模式（MySQL 8.0 之前默认）
- 交错模式(MySQL 8.0 之后默认)
交错模式下，所有的“INSERT-LIKE”语句（所有的插入语句，包括：INSERT、REPLACE、INSERT…SELECT、REPLACE…SELECT、LOAD DATA等）都不使用表级锁，使用的是轻量级互斥锁实现，多条插入语句可以并发执行，速度更快，扩展性也更好。
不过，如果你的 MySQL 数据库有主从同步需求并且 Binlog 存储格式为 Statement 的话，不要将 InnoDB 自增锁模式设置为交叉模式，不然会有数据不一致性问题。这是因为并发情况下插入语句的执行顺序就无法得到保障。
如果 MySQL 采用的格式为 Statement ，那么 MySQL 的主从同步实际上同步的就是一条一条的 SQL 语句。
#### MySQL 性能优化
##### 有哪些常见的 SQL 优化手段？
// TODO MISS
### MySQL高性能优化规范建议总结
#### 数据库命名规范
所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）
#### 数据库基本设计规范
- 尽量控制单表数据量的大小，建议控制在 500 万以内
- 禁止在数据库中存储文件（比如图片）这类大的二进制数据
#### 数据库字段设计规范
- 优先选择符合存储需要的最小的数据类型
- 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据
a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中。
b. TEXT 或 BLOB 类型只能使用前缀索引
- 避免使用 ENUM 类型
- 尽可能把所有列定义为 NOT NULL
- 一定不要用字符串存储日期
- 同财务相关的金额类数据必须使用 decimal 类型
- 单表不要包含过多字段
#### 索引设计规范
- 限制每张表上的索引数量,建议单张表索引不超过 5 个
- 禁止使用全文索引
- 禁止给表中的每一列都建立单独的索引
- 常见索引列建议
a. 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列
b. 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段
c. 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好
d. 多表 join 的关联列
- 如何选择索引列的顺序
建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。
a. 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）
b. 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）
c. 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）
- 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）
- 对于频繁的查询优先考虑使用覆盖索引
覆盖索引：就是包含了所有查询字段 (where,select,order by,group by 包含的字段) 的索引
a. 避免 InnoDB 表进行索引的二次查询，也就是回表操作
b. 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。
- 索引 SET 规范
尽量避免使用外键约束
#### 数据库 SQL 开发规范
- 尽量不在数据库做运算，复杂运算需移到业务应用里完成
- 优化对性能影响较大的 SQL 语句
- 充分利用表上已经存在的索引
a. 避免使用双%号的查询条件
b. 一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。
c. 在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧
d. 使用 left join 或 not exists 来优化 not in 操作
- 禁止使用 SELECT * 必须使用 SELECT <字段列表> 查询
- 禁止使用不含字段列表的 INSERT 语句
- 建议使用预编译语句进行数据库操作
- 避免数据类型的隐式转换
- 避免使用子查询，可以把子查询优化为 join 操作
通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。子查询性能差的原因： 子查询的结果集无法使用索引。
- 避免使用 JOIN 关联太多的表
对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。关联表建议不超过 5 个。
- 减少同数据库的交互次数
- 对应同一列进行 or 判断时，使用 in 代替 or
in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。
- WHERE 从句中禁止对列进行函数转换和计算
- 在明显不会有重复值时使用 UNION ALL 而不是 UNION
UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作
- 拆分复杂的大 SQL 为多个小 SQL
- 程序连接不同的数据库使用不同的账号，禁止跨库查询
#### 数据库操作行为规范
- 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作
a. 大批量操作可能会造成严重的主从延迟
b. binlog 日志为 row 格式时会产生大量的日志
c. 避免产生大事务操作
- 对于大表使用 pt-online-schema-change 修改表结构
pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。
- 禁止为程序使用的账号赋予 super 权限
- 对于程序连接数据库账号,遵循权限最小原则
### 重要知识点
#### MySQL索引详解
##### 索引底层数据结构选型
###### Hash 表
MySQL 的 InnoDB 存储引擎不直接支持常规的哈希索引，但是，InnoDB 存储引擎中存在一种特殊的“自适应哈希索引”（Adaptive Hash Index），自适应哈希索引并不是传统意义上的纯哈希索引，而是结合了 B+Tree 和哈希索引的特点，以便更好地适应实际应用中的数据访问模式和性能需求。自适应哈希索引的每个哈希桶实际上是一个小型的 B+Tree 结构。这个 B+Tree 结构可以存储多个键值对，而不仅仅是一个键。这有助于减少哈希冲突链的长度，提高了索引的效率。
既然哈希表这么快，为什么 MySQL 没有使用其作为索引的数据结构呢？ 主要是因为 Hash 索引不支持顺序和范围查询。假如我们要对表中的数据进行排序或者进行范围查询，那 Hash 索引可就不行了。并且，每次 IO 只能取一个。
###### B树
B 树& B+树两者有何异同呢？
- B 树的所有节点既存放键(key) 也存放数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。
- 在 B 树中进行范围查询时，首先找到要查找的下限，然后对 B 树进行中序遍历，直到找到查找的上限；而 B+树的范围查询，只需要对链表进行遍历即可。

在 MySQL 中，MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是，两者的实现方式不太一样。（下面的内容整理自《Java 工程师修炼之道》）
- MyISAM 引擎中，B+Tree 叶节点的 data 域存放的是数据记录的地址。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引（非聚集索引）”。
- InnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（聚集索引）”，而其余的索引都作为 辅助索引 ，辅助索引的 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。
##### 索引类型总结
###### 按照数据结构维度划分：
- BTree 索引：MySQL 里默认和最常用的索引类型。只有叶子节点存储 value，非叶子节点只有指针和 key。存储引擎 MyISAM 和 InnoDB 实现 BTree 索引都是使用 B+Tree
- 哈希索引：类似键值对的形式，一次即可定位
- RTree 索引：一般不会使用，仅支持 geometry 数据类型，优势在于范围查找，效率较低，通常使用搜索引擎如 ElasticSearch 代替
- 全文索引：对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替
###### 按照底层存储方式角度划分：
- 聚簇索引（聚集索引）：索引结构和数据一起存放的索引，InnoDB 中的主键索引就属于聚簇索引。
- 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，二级索引(辅助索引)就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。
###### 按照应用维度划分：
- 覆盖索引：一个索引包含（或者说覆盖）所有需要查询的字段的值。
- 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。
- 全文索引
###### MySQL 8.x 中实现的索引新特性：
- 隐藏索引：也称为不可见索引，不会被优化器使用，但是仍然需要维护，通常会软删除和灰度发布的场景中使用。主键不能设置为隐藏（包括显式设置或隐式设置）。
- 降序索引：之前的版本就支持通过 desc 来指定索引为降序，但实际上创建的仍然是常规的升序索引。直到 MySQL 8.x 版本才开始真正支持降序索引。另外，在 MySQL 8.x 版本中，不再对 GROUP BY 语句进行隐式排序。
- 函数索引：从 MySQL 8.0.13 版本开始支持在索引中使用函数或者表达式的值，也就是在索引中可以包含函数或者表达式。
##### 主键索引(Primary Key)
如果没有设置主键，InnoDB 将会自动创建一个 6Byte 的自增主键。
##### 二级索引
二级索引（Secondary Index）的叶子节点存储的数据是主键的值，也就是说，通过二级索引可以定位主键的位置，二级索引又称为辅助索引/非主键索引。
唯一索引，普通索引，前缀索引，全文索引等索引都属于二级索引。
##### 聚簇索引
聚簇索引（Clustered Index）即索引结构和数据一起存放的索引，并不是一种单独的索引类型。InnoDB 中的主键索引就属于聚簇索引。
优点：
- 查询速度非常快
- 对排序查找和范围查找优化
缺点：
- 依赖于有序的数据：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
- 更新代价大：如果对索引列的数据被修改时，那么对应的索引也将会被修改，而且聚簇索引的叶子节点还存放着数据，修改代价肯定是较大的，所以对于主键索引来说，主键一般都是不可被修改的。
##### 非聚簇索引
非聚簇索引(Non-Clustered Index)即索引结构和数据分开存放的索引，并不是一种单独的索引类型。二级索引(辅助索引)就属于非聚簇索引。
优点：
更新代价比聚簇索引要小 。非聚簇索引的更新代价就没有聚簇索引那么大了，非聚簇索引的叶子节点是不存放数据的。
缺点：
- 对排序查找和范围查找优化
- 可能会二次查询(回表)
##### 覆盖索引
如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为 覆盖索引（Covering Index）。
##### 联合索引
使用表中的多个字段创建索引，就是 联合索引，也叫 组合索引 或 复合索引。
###### 最左前缀匹配原则
最左匹配原则会一直向右匹配，直到遇到范围查询（如 >、<）为止。但对于 >=、<=、BETWEEN 以及前缀匹配 LIKE 的范围查询，不会停止匹配
- a > 1 and b = 2, 只有 a 字段用到了联合索引进行索引查询，而 b 字段并没有使用到联合索引。
- a >= 1 and b = 2, a 和 b 字段都用到了联合索引进行索引查询。
- name like 'j%' and age = 22, a 和 b 字段都用到了联合索引进行索引查询
MySQL 8.0.13 版本引入了索引跳跃扫描（Index Skip Scan，简称 ISS），它可以在某些索引查询场景下提高查询效率。在没有 ISS 之前，不满足最左前缀匹配原则的联合索引查询中会执行全表扫描。而 ISS 允许 MySQL 在某些情况下避免全表扫描，即使查询条件不符合最左前缀。不过，这个功能比较鸡肋， 和 Oracle 中的没法比，
##### 索引下推
索引下推（Index Condition Pushdown，简称 ICP） 是 MySQL 5.6 版本中提供的一项索引优化功能，它允许存储引擎在索引遍历过程中，执行部分 WHERE字句的判断条件，直接过滤掉不满足条件的记录，从而减少回表次数，提高查询效率。
- 没有索引下推之前，即使 zipcode 字段利用索引可以帮助我们快速定位到 zipcode = '431200' 的用户，但我们仍然需要对每一个找到的用户进行回表操作，获取完整的用户数据，再去判断 MONTH(birthdate) = 3。
- 有了索引下推之后，存储引擎会在使用zipcode 字段索引查找zipcode = '431200' 的用户时，同时判断MONTH(birthdate) = 3。这样，只有同时满足条件的记录才会被返回，减少了回表次数。
索引下推的下推其实就是指将部分上层（Server 层）负责的事情，交给了下层（存储引擎层）去处理。
###### 索引下推应用范围
- 对于 InnoDB 表，仅用于非聚簇索引。索引下推的目标是减少全行读取次数，从而减少 I/O 操作。对于 InnoDB 聚集索引，完整的记录已经读入 InnoDB 缓冲区。在这种情况下使用索引下推 不会减少 I/O。
- 子查询不能使用索引下推，因为子查询通常会创建临时表来处理结果，而这些临时表是没有索引的。
##### 正确使用索引的一些建议
- 不为 NULL 的字段：索引字段的数据应该尽量不为 NULL。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。
- 被频繁更新的字段应该慎重建立索引
- 注意避免冗余索引 冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。
- 字符串类型的字段使用前缀索引代替普通索引
- 避免索引失效
a. SELECT * 不会直接导致索引失效, 但它可能会带来一些其他的性能问题比如造成网络传输和数据处理的浪费、无法使用索引覆盖;
b. 查询条件中使用 OR，且 OR 的前后条件中有一个列没有索引，涉及的索引都不会被使用到;
- 删除长期未使用的索引
#### MySQL三大日志(binlog、redo log和undo log)详解
MySQL 日志 主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中，比较重要的还要属二进制日志 binlog（归档日志，Server层）和事务日志 redo log（重做日志，存储引擎层）和 undo log（回滚日志）。
##### redo log
MySQL 实例挂了或宕机了，重启时，InnoDB 存储引擎会使用 redo log 恢复数据，保证数据的持久性与完整性。
每条 redo 记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成。
a. MySQL 中数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 Buffer Pool 中。
b. 后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。
c. 更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新。
d. 然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里。
理想情况，事务一提交就会进行刷盘操作，但实际上，刷盘的时机是根据策略来进行的。
###### 刷盘时机
- 事务提交
- log buffer 空间不足时, redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动刷盘。
- 事务日志缓冲区满：InnoDB 使用一个事务日志缓冲区（transaction log buffer）来暂时存储事务的重做日志条目。
- Checkpoint（检查点）：InnoDB 定期会执行检查点操作，将内存中的脏数据（已修改但尚未写入磁盘的数据）刷新到磁盘，并且会将相应的重做日志一同刷新，以确保数据的一致性。
- 后台刷新线程：InnoDB 启动了一个后台线程，负责周期性（每隔 1 秒）地将脏页（已修改但尚未写入磁盘的数据页）刷新到磁盘，并将相关的重做日志一同刷新。
- 正常关闭服务器：MySQL 关闭的时候，redo log 都会刷入到磁盘里去。根据 MySQL 配置的刷盘策略的不同，MySQL 宕机之后可能会存在轻微的数据丢失问题。
根据 MySQL 配置的刷盘策略的不同，MySQL 宕机之后可能会存在轻微的数据丢失问题。
- 0：设置为 0 的时候，表示每次事务提交时不进行刷盘操作。这种方式性能最高，但是也最不安全，因为如果 MySQL 挂了或宕机了，可能会丢失最近 1 秒内的事务。
- 1：设置为 1 的时候，表示每次事务提交时都将进行刷盘操作。这种方式性能最低，但是也最安全，因为只要事务提交成功，redo log 记录就一定在磁盘里，不会有任何数据丢失。为了保证事务的持久性，我们必须将其设置为 1。
- 2：设置为 2 的时候，表示每次事务提交时都只把 log buffer 里的 redo log 内容写入 page cache（文件系统缓存）。page cache 是专门用来缓存文件的，这里被缓存的文件就是 redo log 文件。这种方式的性能和安全性都介于前两者中间。
###### 日志文件组
硬盘上存储的 redo log 日志文件不只一个，而是以一个日志文件组的形式出现的，每个的redo日志文件大小都是一样的。
在这个日志文件组中还有两个重要的属性，分别是 write pos、checkpoint
- write pos 是当前记录的位置，一边写一边后移
- checkpoint 是当前要擦除的位置，也是往后推移
每次刷盘 redo log 记录到日志文件组中，write pos 位置就会后移更新。
每次 MySQL 加载日志文件组恢复数据时，会清空加载过的 redo log 记录，并把 checkpoint 后移更新。
write pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。
如果 write pos 追上 checkpoint ，表示日志文件组满了，这时候不能再写入新的 redo log 记录，MySQL 得停下来，清空一些记录，把 checkpoint 推进一下。
MySQL 8.0.30 及之后的版本中，这两个变量已被废弃，即使被指定也是用来计算 innodb_redo_log_capacity 的值。而日志文件组的文件数则固定为 32，文件大小则为 innodb_redo_log_capacity / 32 。
###### redo log 小结
用 redo log 形式记录修改内容，性能会远远超过刷数据页的方式，这也让数据库的并发能力更强。
##### binlog
redo log 它是物理日志，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎。
binlog 是逻辑日志，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于MySQL Server 层。
binlog 会记录所有涉及更新数据的逻辑操作，并且是顺序写。
###### 记录格式
- statement, 记录的内容是SQL语句原文
- row, 记录的内容不再是简单的SQL语句了，还包含操作的具体数据。row格式记录的内容看不到详细信息，要通过mysqlbinlog工具解析出来。但是这种格式，需要更大的容量来记录，比较占用空间，恢复与同步时会更消耗 IO 资源，影响执行速度。
- mixed
###### 写入机制
binlog 的写入时机也非常简单，事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到 binlog 文件中。
- write，是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快
- fsync，才是将数据持久化到磁盘的操作
为了安全起见，可以设置为1，表示每次提交事务都会执行fsync，就如同 redo log 日志刷盘流程 一样。还有一种折中方式，可以设置为N(N>1)，表示每次提交事务都write，但累积N个事务后才fsync。
因为一个事务的 binlog 不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为binlog cache。如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。
##### 两阶段提交
- redo log（重做日志）让 InnoDB 存储引擎拥有了崩溃恢复能力
- binlog（归档日志）保证了 MySQL 集群架构的数据一致性
以基本的事务为单位，redo log 在事务执行过程中可以不断写入，而 binlog 只有在提交事务时才写入，所以 redo log 与 binlog 的写入时机不一样。
- MySQL 根据 redo log 日志恢复数据时，发现 redo log 还处于prepare阶段，并且没有对应 binlog 日志，就会回滚该事务。
- redo log 是处于prepare阶段，但是能通过事务id找到对应的 binlog 日志，所以 MySQL 认为是完整的，就会提交事务恢复数据
##### undo log
每一个事务对数据的修改都会被记录到 undo log ，当执行事务过程中出现错误或者需要执行回滚操作的话，MySQL 可以利用 undo log 将数据恢复到事务开始之前的状态。
undo log 属于逻辑日志，记录的是 SQL 语句，比如说事务执行一条 DELETE 语句，那 undo log 就会记录一条相对应的 INSERT 语句。同时，undo log 的信息也会被记录到 redo log 中，因为 undo log 也要实现持久性保护。
undo-log 本身是会被删除清理的，例如 INSERT 操作，在事务提交之后就可以清除掉了；UPDATE/DELETE 操作在事务提交不会立即删除，会加入 history list，由后台线程 purge 进行清理。
undo log 是采用 segment（段）的方式来记录的，每个 undo 操作在记录的时候占用一个 undo log segment（undo 日志段），undo log segment 包含在 rollback segment（回滚段）中。事务开始时，需要为其分配一个 rollback segment。每个 rollback segment 有 1024 个 undo log segment，这有助于管理多个并发事务的回滚需求。
MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改
##### 总结
MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。
MySQL 数据库的数据备份、主备、主主、主从都离不开 binlog，需要依靠 binlog 来同步数据，保证数据一致性。
#### InnoDB存储引擎对MVCC的实现
##### 多版本并发控制 (Multi-Version Concurrency Control)
通过在每个数据行上维护多个版本的数据来实现的。当一个事务要对数据库中的数据进行修改时，MVCC 会为该事务创建一个数据快照，而不是直接修改实际的数据行。
###### 读操作（SELECT）
- 对于读取操作，事务会查找符合条件的数据行，并选择符合其事务开始时间的数据版本进行读取。
- 如果某个数据行有多个版本，事务会选择不晚于其开始时间的最新版本，确保事务只读取在它开始之前已经存在的数据。
- 事务读取的是快照数据，因此其他并发事务对数据行的修改不会影响当前事务的读取操作。
###### 写操作（INSERT、UPDATE、DELETE）
- 对于写操作，事务会为要修改的数据行创建一个新的版本，并将修改后的数据写入新版本
- 新版本的数据会带有当前事务的版本号，以便其他事务能够正确读取相应版本的数据
- 原始版本的数据仍然存在，供其他事务使用快照读取，这保证了其他事务不受当前事务的写操作影响
###### 事务提交和回滚
- 当一个事务提交时，它所做的修改将成为数据库的最新版本，并且对其他事务可见
- 当一个事务回滚时，它所做的修改将被撤销，对其他事务不可见
###### 版本的回收
MVCC 通过创建数据的多个版本和使用快照读取来实现并发控制。读操作使用旧版本数据的快照，写操作创建新版本，并确保原始版本仍然可用。这样，不同的事务可以在一定程度上并发执行，而不会相互干扰，从而提高了数据库的并发性能和数据一致性。
##### 一致性非锁定读和锁定读
###### 一致性非锁定读
通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号 + 1 或者更新时间戳。
在 InnoDB 存储引擎中，多版本控制 (multi versioning) 就是对非锁定读的实现。如果读取的行正在执行 DELETE 或 UPDATE 操作，这时读取操作不会去等待行上锁的释放。相反地，InnoDB 存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)
在 Repeatable Read 和 Read Committed 两个隔离级别下，如果是执行普通的 select 语句（不包括 select ... lock in share mode ,select ... for update）则会使用 一致性非锁定读（MVCC）。并且在 Repeatable Read 下 MVCC 实现了可重复读和防止部分幻读
###### 锁定读
在锁定读下，读取的是数据的最新版本，这种读也被称为 当前读（current read）。锁定读会对读取到的记录加锁
如果两次查询中间有其它事务插入数据，就会产生幻读。所以， InnoDB 在实现Repeatable Read 时，如果执行的是当前读，则会对读取的记录使用 Next-key Lock ，来防止其它事务在间隙间插入数据
##### InnoDB 对 MVCC 的实现
###### 隐藏字段
- DB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务 id。此外，delete 操作在内部被视为更新，只不过会在记录头 Record header 中的 deleted_flag 字段将其标记为已删除
- DB_ROLL_PTR（7字节） 回滚指针，指向该行的 undo log 。如果该行未被更新，则为空
- DB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引
###### ReadView
用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”
###### undo-log
主要有两个作用
- 当事务回滚时用于将数据恢复到修改前的样子
- 另一个作用是 MVCC ，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过 undo log 读取之前的版本数据，以此实现非锁定读
在 InnoDB 存储引擎中 undo log 分为两种：insert undo log 和 update undo log：
- insert undo log：指在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对其他事务不可见，故该 undo log 可以在事务提交后直接删除。不需要进行 purge 操作
- update undo log：update 或 delete 操作中产生的 undo log。该 undo log可能需要提供 MVCC 机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge线程 进行最后的删除
不同事务或者相同事务的对同一记录行的修改，会使该记录行的 undo log 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。
###### 数据可见性算法
##### RC 和 RR 隔离级别下 MVCC 的差异
- 在 RC 隔离级别下的 每次select 查询前都生成一个Read View (m_ids 列表)
- 在 RR 隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids 列表）
#### SQL语句在MySQL中的执行过程
##### MySQL 基础架构分析
- 连接器 连接器主要和身份认证和权限相关的功能相关
- 查询缓存 查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集
- 分析器 
a. 词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等
b. 语法分析，主要就是判断你输入的 SQL 是否正确，是否符合 MySQL 的语法。
- 优化器 优化器的作用就是它认为的最优的执行方案去执行
- 执行器 执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果
##### 语句分析
###### 更新语句
- 先查询到张三这一条数据，不会走查询缓存，因为更新语句会导致与该表相关的查询缓存失效
- 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交
- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态
- 更新完成
##### MySQL回复处理过程
- 判断 redo log 是否完整，如果判断是完整的，就立即提交
- 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务
##### 总结
- 查询语句的执行流程如下：权限校验（如果命中缓存）--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎
- 更新语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log(prepare 状态)--->binlog--->redo log(commit 状态)
#### MySQL查询缓存详解
##### MySQL 缓存机制
###### 缓存规则
- 查询缓存会将查询语句和结果集保存到内存（一般是 key-value 的形式，key 是查询语句，value 是查询的结果集），下次再查直接从内存中取
- 缓存的结果是通过 sessions 共享的，所以一个 client 查询的缓存结果，另一个 client 也可以使用
- SQL 必须完全一致才会导致查询缓存命中。检查查询缓存时，MySQL Server 不会对 SQL 做任何处理，它精确的使用客户端传来的查询。
- 不缓存查询中的子查询结果集，仅缓存查询最终结果集
- 不确定的函数将永远不会被缓存, 比如 now()、curdate()、last_insert_id()、rand() 等
- 不缓存产生告警（Warnings）的查询
- 太大的结果集不会被缓存 (< query_cache_limit)
- 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存
- 缓存建立之后，MySQL 的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效
- MySQL 缓存在分库分表环境下是不起作用的
- 不缓存使用 SQL_NO_CACHE 的查询
```
SELECT SQL_CACHE id, name FROM customer;# 会缓存
SELECT SQL_NO_CACHE id, name FROM customer;# 不会缓存
```
###### 缓存机制中的内存管理
一个结果集的缓存通过链表把这些 block 串起来。block 最短长度为 query_cache_min_res_unit
分配内存块需要先锁住空间块，所以操作很慢，MySQL 会尽量避免这个操作，选择尽可能小的内存块，如果不够，继续申请，如果存储完时有空余则释放多余的。
##### MySQL 查询缓存对性能的影响
- 当向某个表写入数据的时候，必须将这个表所有的缓存设置为失效，如果缓存空间很大，则消耗也会很大，可能使系统僵死一段时间，因为这个操作是靠全局锁操作来保护的。
- 对 InnoDB 表，当修改一个表时，设置了缓存失效，但是多版本特性会暂时将这修改对其他事务屏蔽，在这个事务提交之前，所有查询都无法使用缓存，直到这个事务被提交，所以长时间的事务，会大大降低查询缓存的命中
##### 总结
查询缓存的适用场景
- 表数据修改不频繁、数据较静态
- 查询（Select）重复度高
- 查询结果集小于 1 MB
实际项目中，更建议使用本地缓存（比如 Caffeine）或者分布式缓存（比如 Redis） ，性能更好，更通用一些
#### MySQL执行计划分析
##### type（重要）
- system：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。
- const：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。
- eq_ref：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了 system 与 const 之外最好的 join 方式，常用于使用主键或唯一索引的所有字段作为连表条件。
- ref：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。
- index_merge：当查询条件使用了多个索引时，表示开启了 Index Merge 优化，此时执行计划中的 key 列列出了使用到的索引。
- range：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。
- index：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。
- ALL：全表扫描。
##### Extra（重要）
- Using filesort：在排序时使用了外部的索引排序，没有用到表内索引进行排序。
- Using temporary：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。
- Using index：表明查询使用了覆盖索引，不用回表，查询效率非常高。
- Using index condition：表示查询优化器选择使用了索引条件下推这个特性。
- Using where：表明查询使用了 WHERE 子句进行条件过滤。一般在没有使用到索引的时候会出现。
- Using join buffer (Block Nested Loop)：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。
当 Extra 列包含 Using filesort 或 Using temporary 时，MySQL 的性能可能会存在问题，需要尽可能避免。
#### MySQL自增主键一定是连续的吗
##### 自增值保存在哪里？
- MyISAM 引擎的自增值保存在数据文件中
- InnoDB 引擎的自增值，其实是保存在了内存里，并没有持久化。第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。到了 MySQL 8.0 版本后，自增值的变更记录被放在了 redo log 中，提供了自增值持久化的能力 ，也就是实现了“如果发生重启，表的自增值可以根据 redo log 恢复为 MySQL 重启前的值”
##### 自增值不连续的场景
- 自增初始值和自增步长设置不为 1
- 唯一键冲突
- 事务回滚
- 批量插入数据，每次申请到的自增 id 个数都是上一次的两倍
#### MySQL日期类型选择建议
##### Datetime 和 Timestamp 之间的抉择
推荐 Timestamp
###### 时区信息
DateTime 类型是没有时区信息的（时区无关）
Timestamp 和时区有关
###### 占用空间
DateTime 的范围是 5~8 字节，Timestamp 的范围是 4~7 字节
###### 表示范围
Timestamp 表示的时间范围更小，只能到 2038 年
###### 性能
由于 TIMESTAMP 需要根据时区进行转换，所以从毫秒数转换到 TIMESTAMP 时，不仅要调用一个简单的函数，还要调用操作系统底层的系统函数。这个系统函数为了保证操作系统时区的一致性，需要进行加锁操作，这就降低了效率。
为了避免 TIMESTAMP 的时区转换问题，建议使用指定的时区，而不是依赖于操作系统时区。
#### MySQL隐式转换造成索引失效
##### 分析和总结
- 当操作符左右两边的数据类型不一致时，会发生隐式转换
- 当 where 查询操作符左边为数值类型时发生了隐式转换，那么对效率影响不大，但还是不推荐这么做
- 当 where 查询操作符左边为字符类型时发生了隐式转换，那么会导致索引失效，造成全表扫描效率极低
- 字符串转换为数值类型时，非数字开头的字符串会转化为0，以数字开头的字符串会截取从第一个字符到第一个非数字内容为止的值为转化结果
## Redis
### 缓存基础常见面试题总结
// TODO MISS
### Redis常见面试题总结(上)
#### Redis 基础
##### Redis 为什么这么快？
- Redis 基于内存，内存的访问速度比磁盘快很多
- Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型，主要是单线程事件循环和 IO 多路复用（Redis 线程模式后面会详细介绍到）
- Redis 内置了多种优化过后的数据类型/结构实现，性能非常高
- Redis 通信协议实现简单且解析高效
##### 分布式缓存常见的技术选型方案有哪些？
- Tendis
- Gragonfly
- KeyDB
##### 说一下 Redis 和 Memcached 的区别和共同点
- 数据类型：Redis 支持更丰富的数据类型
- 数据持久化：Redis 支持数据的持久化
- 集群模式支持：Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据
- 线程模型：Memcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的多路 IO 复用模型。 （Redis 6.0 针对网络数据的读写引入了多线程）
- 特性支持：Redis 支持发布订阅模型、Lua 脚本、事务等功能
- 过期数据删除：Memcached 过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除
##### 如何基于 Redis 实现延时任务？
- Redis 过期事件监听，时效性较差、丢消息、多服务实例下消息重复消费等问题，不被推荐使用。
- Redisson 内置的延时队列 
##### String 还是 Hash 存储对象数据更好呢？
- String 存储的是序列化后的对象数据，存放的是整个对象。Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改或者添加部分字段，节省网络流量。如果对象中某些字段需要经常变动或者经常需要单独查询对象中的个别字段信息，Hash 就非常适合。如用户购物车信息。
- String 存储相对来说更加节省内存，缓存相同数量的对象数据，String 消耗的内存约是 Hash 的一半。并且，存储具有多层嵌套的对象时也方便很多。如果系统对性能和资源消耗非常敏感的话，String 就非常适合。
在绝大部分情况，我们建议使用 String 来存储对象数据
##### 使用 Redis 实现一个排行榜怎么做？
Redis 中有一个叫做 Sorted Set （有序集合）的数据类型经常被用在各种排行榜的场景，比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等
##### Redis 的有序集合底层为什么要用跳表，而不用平衡树、红黑树或者 B+树？
- 平衡树 vs 跳表：平衡树的插入、删除和查询的时间复杂度和跳表一样都是 O(log n)。跳表诞生的初衷就是为了克服平衡树的一些缺点。跳表使用概率平衡而不是严格强制的平衡。
- 红黑树 vs 跳表：按照区间来查找数据这个操作，红黑树的效率没有跳表高
- B+树 vs 跳表：对于索引不需要通过 B+树这种方式进行维护，只需按照概率进行随机维护即可，节约内存。而且使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并
##### 使用 Set 实现抽奖系统怎么做？
- SADD key member1 member2 ...：向指定集合添加一个或多个元素
- SPOP key count：随机移除并获取指定集合中一个或多个元素，适合不允许重复中奖的场景
- SRANDMEMBER key count : 随机获取指定集合中指定数量的元素，适合允许重复中奖的场景
##### 使用 Bitmap 统计活跃用户怎么做？
你可以将 Bitmap 看作是一个存储二进制数字（0 和 1）的数组，数组中每个元素的下标叫做 offset（偏移量）
##### 使用 HyperLogLog 统计页面 UV 怎么做？
- PFADD key element1 element2 ...：添加一个或多个元素到 HyperLogLog 中
- PFCOUNT key1 key2：获取一个或者多个 HyperLogLog 的唯一计数
##### Redis 后台线程了解吗？
- 通过 bio_close_file 后台线程来释放 AOF / RDB 等过程中产生的临时文件资源
- 通过 bio_aof_fsync 后台线程调用 fsync 函数将系统内核缓冲区还未同步到到磁盘的数据强制刷到磁盘（ AOF 文件）
- 通过 bio_lazy_free后台线程释放大对象（已删除）占用的内存空间
##### Redis 过期 key 删除策略了解么？
- 惰性删除：只会在取出/查询 key 的时候才对数据进行过期检查。这种方式对 CPU 最友好，但是可能会造成太多过期 key 没有被删除
- 定期删除：周期性地随机从设置了过期时间的 key 中抽查一批，然后逐个检查这些 key 是否过期，过期就删除 key。
- 延迟队列：把设置过期时间的 key 放到一个延迟队列里，到期之后就删除 key
- 定时删除：每个设置了过期时间的 key 都会在设置的时间到达时立即被删除
##### Redis 采用的那种删除策略呢？
Redis 采用的是 定期删除+惰性/懒汉式删除 结合的策略。
定期删除还会受到执行时间和过期 key 的比例的影响
- 执行时间已经超过了阈值，那么就中断这一次定期删除循环，以避免使用过多的 CPU 时间
- 如果这一批过期的 key 比例超过一个比例，就会重复执行此删除流程，以更积极地清理过期 key。相应地，如果过期的 key 比例低于这个比例，就会中断这一次定期删除循环，避免做过多的工作而获得很少的内存回收。
### Redis常见面试题总结(下)
#### Redis 事务
##### 什么是 Redis 事务？
除了不满足原子性和持久性之外，事务中的每条命令都会与 Redis 服务器进行网络交互，这是比较浪费资源的行为。
MULTI 命令后可以输入多个命令，Redis 不会立即执行这些命令，而是将它们放到队列，当调用了 EXEC 命令后，再执行所有的命令。
通过WATCH 命令监听指定的 Key，当调用 EXEC 命令执行事务时，如果一个被 WATCH 命令监视的 Key 被 其他客户端/Session 修改的话，整个事务都不会被执行。
不过，如果 WATCH 与 事务 在同一个 Session 里，并且被 WATCH 监视的 Key 被修改的操作发生在事务内部，这个事务是可以被执行成功的。
##### Redis 事务支持原子性吗？
Redis 官网也解释了自己为啥不支持回滚。简单来说就是 Redis 开发者们觉得没必要支持回滚，这样更简单便捷并且性能更好。Redis 开发者觉得即使命令执行错误也应该在开发过程中就被发现而不是生产过程中。
##### Redis 事务支持持久性吗？
- 快照（snapshotting，RDB）
- 只追加文件（append-only file, AOF）
- RDB 和 AOF 的混合持久化(Redis 4.0 新增)
AOF 持久化的fsync策略为 no、everysec 时都会存在数据丢失的情况 。always 下可以基本是可以满足持久性要求的，但性能太差，实际开发过程中不会使用。
Redis 事务的持久性也是没办法保证的
##### 如何解决 Redis 事务的缺陷？
严格来说的话，通过 Lua 脚本来批量执行 Redis 命令实际也是不完全满足原子性的。
如果想要让 Lua 脚本中的命令全部执行，必须保证语句语法和命令都是对的。
另外，Redis 7.0 新增了 Redis functions 特性，你可以将 Redis functions 看作是比 Lua 更强大的脚本。
#### Redis 性能优化（重要）
- 使用批量操作减少网络传输
a. 原生批量操作命令,  但在 Redis 官方提供的分片集群解决方案 Redis Cluster 下，使用这些原生批量操作命令可能会存在一些小问题需要解决。
b. pipeline,  需要注意控制一次批量操作的 元素个数(例如 500 以内，实际也和元素字节数有关)，避免网络传输的数据量过大。与MGET、MSET等原生批量操作命令一样，pipeline 同样在 Redis Cluster 上使用会存在一些小问题。pipeline 不适用于执行顺序有依赖关系的一批命令。就比如说，你需要将前一个命令的结果给后续的命令使用，pipeline 就没办法满足你的需求了
原生批量操作命令和 pipeline 的是有区别的，使用的时候需要注意：
- 原生批量操作命令是原子操作，pipeline 是非原子操作。两个不同的事务不会同时运行，而 pipeline 可以同时以交错方式执行。
- pipeline 可以打包不同的命令，原生批量操作命令不可以。Redis 事务中每个命令都需要发送到服务端，而 Pipeline 只需要发送一次，请求次数更少
- 原生批量操作命令是 Redis 服务端支持实现的，而 pipeline 需要服务端和客户端的共同实现
c. Lua 脚本
Lua 脚本同样支持批量操作多条命令。一段 Lua 脚本可以视作一条命令执行，可以看作是 原子操作 。也就是说，一段 Lua 脚本执行过程中不会有其他脚本或 Redis 命令同时执行，保证了操作不会被其他指令插入或打扰，这是 pipeline 所不具备的。
并且，Lua 脚本中支持一些简单的逻辑处理比如使用命令读取值并在 Lua 脚本中进行处理，这同样是 pipeline 所不具备的。
缺陷:
- 使用了 Lua 脚本，也不能实现类似数据库回滚的原子性
- Redis Cluster 下 Lua 脚本的原子操作也无法保证了，原因同样是无法保证所有的 key 都在同一个 hash slot（哈希槽）上
###### 大量 key 集中过期问题
开启 lazy-free（惰性删除/延迟释放） 。lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程
###### Redis bigkey（大 Key）
####### 什么是 bigkey？
如果一个 key 对应的 value 所占用的内存比较大，那这个 key 就可以看作是 bigkey
- String 类型的 value 超过 1MB
- 复合类型（List、Hash、Set、Sorted Set 等）的 value 包含的元素超过 5000 个（不过，对于复合类型的 value 来说，不一定包含的元素越多，占用的内存就越多）
####### bigkey 有什么危害？
- 客户端超时阻塞
- 网络阻塞
- 工作线程阻塞
####### 如何发现 bigkey？
- 使用 Redis 自带的 --bigkeys 参数来查找。这个命令会扫描(Scan) Redis 中的所有 key ，会对 Redis 的性能有一点影响。并且，这种方式只能找出每种数据结构 top 1 bigkey（占用内存最大的 String 数据类型，包含元素最多的复合数据类型）。然而，一个 key 的元素多并不代表占用内存也多，需要我们根据具体的业务情况来进一步判断。
- 使用 Redis 自带的 SCAN 命令 
- 借助开源工具分析 RDB 文件
a. redis-rdb-tools：Python 语言写的用来分析 Redis 的 RDB 快照文件用的工具
b. rdb_bigkeys : Go 语言写的用来分析 Redis 的 RDB 快照文件用的工具，性能更好。
- 借助公有云的 Redis 分析服务
####### 如何处理 bigkey？
- 分割 bigkey：将一个 bigkey 分割为多个小 key。例如，将一个含有上万字段数量的 Hash 按照一定策略（比如二次哈希）拆分为多个 Hash
- 手动清理：Redis 4.0+ 可以使用 UNLINK 命令来异步删除一个或多个指定的 key。Redis 4.0 以下可以考虑使用 SCAN 命令结合 DEL 命令来分批次删除
- 采用合适的数据结构：例如，文件二进制数据不使用 String 保存、使用 HyperLogLog 统计页面 UV、Bitmap 保存状态信息（0/1）
- 开启 lazy-free（惰性删除/延迟释放） ：lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程
###### Redis hotkey（热 Key）
####### 什么是 hotkey？
如果一个 key 的访问次数比较多且明显多于其他 key 的话，那这个 key 就可以看作是 hotkey（热 Key）
####### hotkey 有什么危害？
如果突然访问 hotkey 的请求超出了 Redis 的处理能力，Redis 就会直接宕机。这种情况下，大量请求将落到后面的数据库上，可能会导致数据库崩溃。
因此，hotkey 很可能成为系统性能的瓶颈点，需要单独对其进行优化，以确保系统的高可用性和稳定性
####### 如何发现 hotkey？
- 使用 Redis 自带的 --hotkeys 参数来查找。 使用该方案的前提条件是 Redis Server 的 maxmemory-policy 参数设置为 LFU 算法，不然就会出现如下所示的错误。 需要注意的是，hotkeys 参数命令也会增加 Redis 实例的 CPU 和内存消耗（全局扫描），因此需要谨慎使用。
- 使用MONITOR 命令。 由于该命令对 Redis 性能的影响比较大，因此禁止长时间开启 MONITOR（生产环境中建议谨慎使用该命令）
- 借助开源项目。 京东零售的 hotkey 这个项目不光支持 hotkey 的发现，还支持 hotkey 的处理。
- 根据业务情况提前预估。
- 业务代码中记录分析。
- 借助公有云的 Redis 分析服务。
####### 如何解决 hotkey？
- 读写分离：主节点处理写请求，从节点处理读请求
- 使用 Redis Cluster：将热点数据分散存储在多个 Redis 节点上
- 二级缓存：hotkey 采用二级缓存的方式进行处理，将 hotkey 存放一份到 JVM 本地内存中（可以用 Caffeine）
###### 慢查询命令
####### 为什么会有慢查询命令？
- KEYS *：会返回所有符合规则的 key。 
- HGETALL：会返回一个 Hash 中所有的键值对。
- LRANGE：会返回 List 中指定范围内的元素。
- SMEMBERS：返回 Set 中的所有元素。
- SINTER/SUNION/SDIFF：计算多个 Set 的交集/并集/差集。
- ZRANGE/ZREVRANGE：返回指定 Sorted Set 中指定排名范围内的所有元素。时间复杂度为 O(log(n)+m)，n 为所有元素的数量， m 为返回的元素数量，当 m 和 n 相当大时，O(n) 的时间复杂度更小。
- ZREMRANGEBYRANK/ZREMRANGEBYSCORE：移除 Sorted Set 中指定排名范围/指定 score 范围内的所有元素。时间复杂度为 O(log(n)+m)，n 为所有元素的数量， m 被删除元素的数量，当 m 和 n 相当大时，O(n) 的时间复杂度更小。
随着 n 的增大，执行耗时也会越长。不过， 这些命令并不是一定不能使用，但是需要明确 N 的值。另外，有遍历的需求可以使用 HSCAN、SSCAN、ZSCAN 代替。
####### 如何找到慢查询命令？
使用SLOWLOG GET 命令，指定返回的慢查询命令的数量 SLOWLOG GET N。
#### Redis 生产问题（重要）
- 缓存穿透 根本不存在于缓存中，也不存在于数据库中。
a. 布隆过滤器
b. 接口限流
- 缓存击穿 存在于数据库中，但不存在于缓存中
a. 提前预热（推荐）
b. 加锁（看情况）
- 缓存雪崩 缓存在同一时间大面积的失效，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。
针对 Redis 服务不可用的情况
a. Redis 集群
b. 多级缓存
针对大量缓存同时失效的情况
a. 提前预热（推荐）
b. 持久缓存策略（看情况）
##### 缓存预热如何实现？
- 使用定时任务，比如 xxl-job
- 使用消息队列，比如 Kafka
#####  Cache Aside Pattern（旁路缓存模式）
遇到写请求是这样的：更新数据库，然后直接删除缓存
如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说个解决方案
增加缓存更新重试机制（常用）
如果缓存服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。不过，这里更适合引入消息队列实现异步重试，将删除缓存重试的消息投递到消息队列，然后由专门的消费者来重试，直到成功。虽然说多引入了一个消息队列，但其整体带来的收益还是要更高一些。
### 重要知识点
#### 如何基于Redis实现延时任务
##### Redis 过期事件监听实现延时任务功能的原理？
Redis 中有很多默认的 channel，这些 channel 是由 Redis 本身向它们发送消息的，而不是我们自己编写的代码。其中，__keyevent@0__:expired 就是一个默认的 channel，负责监听 key 的过期事件。也就是说，当一个 key 过期之后，Redis 会发布一个 key 过期的事件到__keyevent@0__:expired这个 channel 中。
我们只需要监听这个 channel，就可以拿到过期的 key 的消息，进而实现了延时任务功能。
这个功能被 Redis 官方称为 keyspace notifications ，作用是实时监控实时监控 Redis 键和值的变化。
##### Redisson 延迟队列原理是什么？有什么优势？
Redisson 使用 zrangebyscore 命令扫描 SortedSet 中过期的元素，然后将这些过期元素从 SortedSet 中移除，并将它们加入到就绪消息列表中。就绪消息列表是一个阻塞队列，有消息进入就会被监听到。这样做可以避免对整个 SortedSet 进行轮询，提高了执行效率。
#### 3种常用的缓存读写策略详解
- Cache Aside Pattern（旁路缓存模式）
缺陷:
a. 首次请求数据一定不在 cache 的问题
b. 写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 
解决办法：
1. 数据库和缓存数据强一致场景：更新 db 的时候同样更新 cache，不过我们需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题。
2. 可以短暂地允许数据库和缓存数据不一致的场景：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。
- Read/Write Through Pattern（读写穿透）
- Write Behind Pattern（异步缓存写入）
很明显，这种方式对数据一致性带来了更大的挑战，比如 cache 数据可能还没异步更新 db 的话，cache 服务可能就就挂掉了。
这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制都用到了这种策略。
Write Behind Pattern 下 db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。
#### Redis 5 种基本数据类型详解
- String（字符串） 简单动态字符串
- List（列表） 双向链表
- Hash（哈希）
- Set（集合）
a. 求交集 SINTERSTORE mySet3 mySet mySet2
b. 求并集 SUNION mySet mySet2
c. 求差集 SDIFF mySet mySet2
- Sorted Set（有序集合）增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。
#### Redis 3 种特殊数据类型详解
- Bitmap （位图）
- HyperLogLog（基数统计）
- Geospatial (地理位置)
#### Redis为什么用跳表实现有序集合
ZSet 有两种不同的实现，分别是 ziplist 和 skiplist，具体使用哪种结构进行存储的规则如下：
- ZSet 保存的键值对数量少于 128 个；
- 每个元素的长度小于 64 字节。
#### Redis持久化机制详解
##### RDB 持久化
- save : 同步保存操作，会阻塞 Redis 主线程
- bgsave : fork 出一个子进程，子进程执行，不会阻塞 Redis 主线程，默认选项。
##### AOF 持久化
###### AOF 工作基本流程是怎样的？
- 命令追加（append）：所有的写命令会追加到 AOF 缓冲区中
- 文件写入（write）：将 AOF 缓冲区的数据写入到 AOF 文件中。这一步需要调用write函数（系统调用），write将数据写入到了系统内核缓冲区之后直接返回了（延迟写）。注意！！！此时并没有同步到磁盘。
- 文件同步（fsync）：AOF 缓冲区根据对应的持久化方式（ fsync 策略）向硬盘做同步操作。这一步需要调用 fsync 函数（系统调用）， fsync 针对单个文件操作，对其进行强制硬盘同步，fsync 将阻塞直到写入磁盘完成后返回，保证了数据持久化。
- 文件重写（rewrite）：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。
- 重启加载（load）：当 Redis 重启时，可以加载 AOF 文件进行数据恢复。
###### AOF 持久化方式有哪些？
从 Redis 7.0.0 开始，Redis 使用了 Multi Part AOF 机制。
- BASE：表示基础 AOF 文件，它一般由子进程通过重写产生，该文件最多只有一个。
- INCR：表示增量 AOF 文件，它一般会在 AOFRW 开始执行时被创建，该文件可能存在多个。
- HISTORY：表示历史 AOF 文件，它由 BASE 和 INCR AOF 变化而来，每次 AOFRW 成功完成时，本次 AOFRW 之前对应的 BASE 和 INCR AOF 都将变为 HISTORY，HISTORY 类型的 AOF 会被 Redis 自动删除。
###### AOF 为什么是在执行完命令之后记录日志？
- 避免额外的检查开销，AOF 记录日志不会对命令进行语法检查；
- 在命令执行完之后再记录，不会阻塞当前的命令执行。
###### AOF 重写了解吗？
当 AOF 变得太大时，Redis 能够在后台自动重写 AOF 产生一个新的 AOF 文件，这个新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。
###### AOF 校验机制了解吗？
通过使用一种叫做 校验和（checksum） 的数字来验证 AOF 文件。这个校验和是通过对整个 AOF 文件内容进行 CRC64 算法计算得出的数字。
#### Redis内存碎片详解
##### 为什么会有 Redis 内存碎片?
- Redis 存储数据的时候向操作系统申请的内存空间可能会大于数据实际需要的存储空间。
- 频繁修改 Redis 中的数据也会产生内存碎片。
##### 如何查看 Redis 内存碎片的信息？
使用 info memory 命令即可查看 Redis 内存相关的信息
mem_fragmentation_ratio （内存碎片率）的值越大代表内存碎片率越严重。我们认为 mem_fragmentation_ratio > 1.5 的话才需要清理内存碎片。 
##### 如何清理 Redis 内存碎片？
直接通过 config set 命令将 activedefrag 配置项设置为 yes 即可
重启节点可以做到内存碎片重新整理。如果你采用的是高可用架构的 Redis 集群的话，你可以将碎片率过高的主节点转换为从节点，以便进行安全重启。
#### Redis常见阻塞原因总结
- O(n) 命令
- SAVE 创建 RDB 快照
- AOF 日志记录阻塞
- AOF 刷盘阻塞
- AOF 重写阻塞
- 查找大 key/删除大 key
- 清空数据库
- 集群扩容 在扩缩容的时候，需要进行数据迁移。而 Redis 为了保证迁移的一致性，迁移所有操作都是同步操作。
- Swap（内存交换） 如果操作系统把 Redis 使用的部分内存换出硬盘，由于内存与硬盘的读写速度差几个数量级，会导致发生交换后的 Redis 性能急剧下降。
```
redis-cli -p 6383 info server | grep process_id
process_id: 4476

cat /proc/4476/smaps | grep Swap
Swap: 0kB
Swap: 0kB
Swap: 4kB
```
如果交换量都是 0KB 或者个别的是 4KB，则正常
预防内存交换的方法
a. 保证机器充足的可用内存
b. 确保所有 Redis 实例设置最大可用内存(maxmemory)，防止极端情况 Redis 内存不可控的增长
c. 降低系统使用 swap 优先级，如echo 10 > /proc/sys/vm/swappiness
- CPU 竞争
- 网络问题
#### Redis集群详解（付费）
// TODO MISS
## Elasticsearch
## MongoDB
### MongoDB常见面试题总结（上）
#### MongoDB 基础
##### MongoDB 是什么？
MongoDB 提供了 面向文档 的存储方式，操作起来比较简单和容易，支持“无模式”的数据建模，可以存储比较复杂的数据类型，是一款非常流行的 文档类型数据库 。
##### MongoDB 的存储结构是什么？
- 文档（Document）：MongoDB 中最基本的单元，由 BSON 键值对（key-value）组成，类似于关系型数据库中的行（Row）
- 集合（Collection）：一个集合可以包含多个文档，类似于关系型数据库中的表（Table）
- 数据库（Database）
###### 文档
与 JSON 相比，BSON 着眼于提高存储和扫描效率。BSON 文档中的大型元素以长度字段为前缀以便于扫描。在某些情况下，由于长度前缀和显式数组索引的存在，BSON 使用的空间会多于 JSON。
###### 集合
MongoDB 集合存在于数据库中，没有固定的结构，也就是 无模式 的，这意味着可以往集合插入不同格式和类型的数据。
###### 数据库
- admin : admin 数据库主要是保存 root 用户和角色。
- local : local 数据库是不会被复制到其他分片的，因此可以用来存储本地单台服务器的任意 collection。
- config : 当 MongoDB 使用分片设置时，config 数据库可用来保存分片的相关信息。
- test : 默认创建的测试库，连接 mongod 服务时，如果不指定连接的具体数据库，默认就会连接到 test 数据库。
##### MongoDB 有什么特点？
- 支持 ACID 事务
- 自带数据压缩功能
- 支持 mapreduce MongoDB 5.0 开始，map-reduce 已经不被官方推荐使用了，替代方案是 聚合管道
- 支持多种类型的索引
- 支持 failover
- 支持分片集群
- 支持存储大文件 对于超过 16MB 的大文件，MongoDB 提供了 GridFS 来进行存储
#### MongoDB 存储引擎
- WiredTiger 存储引擎 WiredTiger 提供文档级并发模型、检查点和数据压缩
对于 NoSQL 数据库来说，绝大部分（比如 HBase、Cassandra、RocksDB）都是基于 LSM 树。WiredTiger 使用的是 B+ 树作为其存储结构。
- In-Memory 存储引擎 将它们保留在内存中以获得更可预测的数据延迟
#### MongoDB 聚合
##### MongoDB 提供了哪几种执行聚合的方法？
- 聚合管道（Aggregation Pipeline）：执行聚合操作的首选方法
阶段操作符用于 db.collection.aggregate 方法里面，数组参数中的第一层。
```
db.orders.aggregate([
   # 第一阶段：$match阶段按status字段过滤文档，并将status等于"A"的文档传递到下一阶段。
    { $match: { status: "A" } },
  # 第二阶段：$group阶段按cust_id字段将文档分组，以计算每个cust_id唯一值的金额总和。
    { $group: { _id: "$cust_id", total: { $sum: "$amount" } } }
])
```
- 单一目的聚合方法（Single purpose aggregation methods）：也就是单一作用的聚合函数比如 count()、distinct()、estimatedDocumentCount()。
#### MongoDB 事务
从 MongoDB 4.2 开始，分布式事务和多文档事务在 MongoDB 中是一个意思。分布式事务是指分片集群和副本集上的多文档事务。从 MongoDB 4.2 开始，多文档事务（无论是在分片集群还是副本集上）也称为分布式事务。
在大多数情况下，多文档事务比单文档写入会产生更大的性能成本。对于大部分场景来说， 非规范化数据模型（嵌入式文档和数组） 依然是最佳选择。也就是说，适当地对数据进行建模可以最大限度地减少对多文档事务的需求。
#### MongoDB 数据压缩
- Snappy 默认情况下，WiredTiger 使用 Snappy 压缩算法（谷歌开源，旨在实现非常高的速度和合理的压缩，压缩比 3 ～ 5 倍）对所有集合使用块压缩，对所有索引使用前缀压缩。如果日志记录小于或等于 128 字节，WiredTiger 不会压缩该记录。
- zlib：高度压缩算法，压缩比 5 ～ 7 倍
- Zstandard（简称 zstd）：Facebook 开源的一种快速无损压缩算法，针对 zlib 级别的实时压缩场景和更好的压缩比，提供更高的压缩率和更低的 CPU 使用率，MongoDB 4.2 开始可用。
### MongoDB常见面试题总结（下）
#### MongoDB 索引
##### MongoDB 支持哪些类型的索引？
- 单字段索引： 建立在单个字段上的索引，索引创建的排序顺序无所谓，MongoDB 可以头/尾开始遍历。
- 复合索引： 建立在多个字段上的索引，也可以称之为组合索引、联合索引。MongoDB 的复合索引遵循左前缀原则。
- 多键索引：MongoDB 的一个字段可能是数组，在对这种字段创建索引时，就是多键索引。MongoDB 会为数组的每个值创建索引。就是说你可以按照数组里面的值做条件来查询，这个时候依然会走索引。
- 哈希索引：按数据的哈希值索引，用在哈希分片集群上。
- 文本索引： 支持对字符串内容的文本搜索查询。文本索引可以包含任何值为字符串或字符串元素数组的字段。一个集合只能有一个文本搜索索引，但该索引可以覆盖多个字段。MongoDB 虽然支持全文索引，但是性能低下，暂时不建议使用。
- 地理位置索引： 基于经纬度的索引，适合 2D 和 3D 的位置查询
- 唯一索引：确保索引字段不会存储重复值。如果集合已经存在了违反索引的唯一约束的文档，则后台创建唯一索引会失败
- TTL 索引：TTL 索引提供了一个过期机制，允许为每一个文档设置一个过期时间，当一个文档达到预设的过期时间之后就会被删除。
从节点的数据删除是由主库删除后产生的 oplog 来做同步
TTL 索引限制：
a. TTL 索引是单字段索引。复合索引不支持 TTL
b. _id字段不支持 TTL 索引
c. 无法在上限集合(Capped Collection)上创建 TTL 索引，因为 MongoDB 无法从上限集合中删除文档
d. 如果某个字段已经存在非 TTL 索引，那么在该字段上无法再创建 TTL 索引
#### MongoDB 高可用
##### 复制集群
主节点机负责整个复制集群的写，从节点可以进行读操作，但默认还是主节点负责整个复制集群的读。主节点发生故障时，自动从从节点中选举出一个新的主节点，确保集群的正常使用，这对于客户端来说是无感知的。
- 主节点：整个集群的写操作入口，接收所有的写操作，并将集合所有的变化记录到操作日志中，即 oplog。主节点挂掉之后会自动选出新的主节点。
- 从节点：从主节点同步数据，在主节点挂掉之后选举新节点。不过，从节点可以配置成 0 优先级，阻止它在选举中成为主节点。
- 仲裁节点：这个是为了节约资源或者多机房容灾用，只负责主节点选举时投票不存数据，保证能有节点获得多数赞成票。
主节点与备节点之间是通过 oplog（操作日志） 来同步数据的。oplog 是 local 库下的一个特殊的 上限集合(Capped Collection) ，用来保存写操作所产生的增量日志，类似于 MySQL 中 的 Binlog。
上限集合类似于定长的循环队列，数据顺序追加到集合的尾部，当集合空间达到上限时，它会覆盖集合中最旧的文档。上限集合的数据将会被顺序写入到磁盘的固定空间内，所以，I/O 速度非常快，如果不建立索引，性能更好。
##### 分片集群[水平扩展]
分片集群数据被均衡的分布在不同分片中， 不仅大幅提升了整个集群的数据容量上限，也将读写的压力分散到不同分片，以解决副本集性能瓶颈的难题。
- Config Servers：配置服务器，本质上是一个 MongoDB 的副本集，负责存储集群的各种元数据和配置，如分片地址、Chunks 等
- Mongos：路由服务，不存具体数据，从 Config 获取集群配置讲请求转发到特定的分片，并且整合分片结果返回给客户端。
- Shard：每个分片是整体数据的一部分子集，从 MongoDB3.6 版本开始，每个 Shard 必须部署为副本集（replica set）架构
###### 什么是分片键？
- 它必须在所有文档中都出现
- 它必须是集合的一个索引，可以是单索引或复合索引的前缀索引，不能是多索引、文本索引或地理空间位置索引。
- 它的大小不能超过 512 字节
###### 如何选择分片键？
- 取值基数 取值基数建议尽可能大
- 取值分布 取值分布建议尽量均匀
- 查询带分片 查询时建议带上分片，使用分片键进行条件查询时，mongos 可以直接定位到具体分片，否则 mongos 需要将查询分发到所有分片，再等待响应返回。
- 避免单调递增或递减 单调递增的 sharding key，数据文件挪动小，但写入会集中，导致最后一篇的数据量持续增大，不断发生迁移，递减同理。
###### 分片策略有哪些？
- 基于范围的分片
优点：Mongos 可以快速定位请求需要的数据，并将请求转发到相应的 Shard 节点中。
缺点：可能导致数据在 Shard 节点上分布不均衡，容易造成读写热点，且不具备写分散性。
适用场景：分片键的值不是单调递增或单调递减、分片键的值基数大且重复的频率低、需要范围查询等业务场景。
- 基于 Hash 值的分片
优点：可以将数据更加均衡地分布在各 Shard 节点中，具备写分散性。
缺点：不适合进行范围查询，进行范围查询时，需要将读请求分发到所有的 Shard 节点。
适用场景：分片键的值存在单调递增或递减、片键的值基数大且重复的频率低、需要写入的数据随机分发、数据读取随机性较大等业务场景。
###### 分片数据如何存储？
数据的增长会让 Chunk 分裂得越来越多。这个时候，各个分片上的 Chunk 数量可能会不平衡。Mongos 中的 均衡器(Balancer) 组件就会执行自动平衡，尝试使各个 Shard 上 Chunk 的数量保持均衡，这个过程就是 再平衡（Rebalance）。默认情况下，数据库和集合的 Rebalance 是开启的。
Chunk 只会分裂，不会合并，即使 chunkSize 的值变大。